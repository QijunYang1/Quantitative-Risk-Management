{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week1-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test （stats - ttest）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-test\n",
    "# stats -- generate distribution & sample\n",
    "\n",
    "from scipy import stats\n",
    "t = np.sqrt(num)*sample.mean/np.sqrt(sample.variance)\n",
    "p = 2*(1-stats.t.cdf(abs(t),df=num-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "# parse the data into the dataframe\n",
    "df=pd.read_csv(\"problem2.csv\")\n",
    "# seperate dependent variable and dependent variable\n",
    "Y=df[\"y\"]\n",
    "x=df[\"x\"]\n",
    "# Our model needs an intercept so we add a column of 1s\n",
    "X=sm.add_constant(x)\n",
    "# build the regression model and fit\n",
    "model = sm.OLS(Y,X)\n",
    "res = model.fit()\n",
    "# get some important statistics for the result\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE -- make it simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Normal\n",
    "\n",
    "# use MLE to do the regression\n",
    "# using the norm distribution from scipy, we can write this likelihood simply as following\n",
    "def _ll_norm(y, X, beta, sigma):\n",
    "    resid=y-np.dot(X, beta)\n",
    "    ll = norm.logpdf(resid, scale = sigma)\n",
    "    return ll\n",
    "# We create a new model class which inherits from GenericLikelihoodModel\n",
    "class Normin(GenericLikelihoodModel):\n",
    "    def __init__(self, endog, exog, **kwds):\n",
    "        self.y=endog\n",
    "        self.X=exog\n",
    "        super().__init__(endog, exog, **kwds)\n",
    "\n",
    "    def nloglikeobs(self, params):\n",
    "        '''This function should return one evaluation of the negative log-likelihood function per observation in your dataset '''\n",
    "        sigma = params[-1]\n",
    "        beta = params[:-1]\n",
    "        ll = _ll_norm(self.endog, self.exog, beta, sigma)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, start_params=None, maxiter=10000, maxfun=5000, **kwds):\n",
    "        ''' start_params: a one-dimensional array of starting values needs to be provided. \n",
    "            The size of this array determines the number of parameters that \n",
    "            will be used in optimization\n",
    "        '''\n",
    "        # we have one additional parameter and we need to add it for summary\n",
    "        self.exog_names.append('sigma')\n",
    "        if start_params == None:\n",
    "            # Reasonable starting values\n",
    "            start_params = np.append(np.zeros(self.exog.shape[1]), 1)\n",
    "            # intercept\n",
    "            start_params[-2] = np.log(self.endog.mean())\n",
    "        self.res = super().fit(start_params=start_params,\n",
    "                                     maxiter=maxiter, maxfun=maxfun,\n",
    "                                     **kwds)\n",
    "        return self.res\n",
    "\n",
    "    def predict(self):\n",
    "        '''calculate the fitted y and residuals'''\n",
    "        y_pre=np.dot(self.X,self.res.params[:2])\n",
    "        err=self.y-y_pre\n",
    "        y_pre=pd.DataFrame(y_pre)\n",
    "        err=pd.DataFrame(err)\n",
    "        df=pd.concat([self.y,self.X.x,y_pre,err],axis=1)\n",
    "        df.columns = ['y','x','y_pre','resid']\n",
    "        return df \n",
    "    \n",
    "    def R2(self):\n",
    "        y_pre=np.dot(self.X,self.res.params[:2])\n",
    "        SSE=sum((self.y-y_pre)**2)\n",
    "        Mean=self.y.mean()\n",
    "        SST=sum((self.y-Mean)**2)\n",
    "        R2=1-SSE/SST\n",
    "\n",
    "        n=self.y.shape[0]\n",
    "        p=1\n",
    "        adj_R2=1-(1-R2)*(n-1)/(n-p-1)\n",
    "        return R2,adj_R2\n",
    "\n",
    "\n",
    "# T\n",
    "def _ll_t(y, X, beta, df, sigma):\n",
    "    resid=y-np.dot(X, beta)\n",
    "    ll = t.logpdf(resid, df, scale=sigma)\n",
    "    return ll\n",
    "    \n",
    "# We create a new model class which inherits from GenericLikelihoodModel\n",
    "class Tin(GenericLikelihoodModel):\n",
    "    def __init__(self, endog, exog, **kwds):\n",
    "        self.y=endog\n",
    "        self.X=exog\n",
    "        super().__init__(endog, exog, **kwds)\n",
    "\n",
    "    def nloglikeobs(self, params):\n",
    "        '''This function should return one evaluation of the negative log-likelihood function per observation in your dataset '''\n",
    "        df = params[-2]\n",
    "        sigma = params[-1]\n",
    "        beta = params[:-2]\n",
    "        ll = _ll_t(self.endog, self.exog, beta, df, sigma)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, start_params=None, maxiter=10000, maxfun=5000, **kwds):\n",
    "        ''' start_params: a one-dimensional array of starting values needs to be provided. \n",
    "            The size of this array determines the number of parameters that \n",
    "            will be used in optimization\n",
    "        '''\n",
    "        \n",
    "        # we have three additional parameters and we need to add it for summary\n",
    "        self.exog_names.append('df')\n",
    "        self.exog_names.append('sigma')\n",
    "        if start_params == None:\n",
    "            # Reasonable starting values\n",
    "            start_params = np.append(np.zeros(self.exog.shape[1]), np.array([50,1]))\n",
    "            # intercept\n",
    "            start_params[-3] = np.log(self.endog.mean())\n",
    "        self.res = super().fit(start_params=start_params,\n",
    "                                     maxiter=maxiter, maxfun=maxfun,\n",
    "                                     **kwds)\n",
    "        return self.res\n",
    "\n",
    "    def predict(self):\n",
    "        '''calculate the fitted y and residuals'''\n",
    "        y_pre=np.dot(self.X,self.res.params[:2])\n",
    "        err=self.y-y_pre\n",
    "        y_pre=pd.DataFrame(y_pre)\n",
    "        err=pd.DataFrame(err)\n",
    "        df=pd.concat([self.y,self.X.x,y_pre,err],axis=1)\n",
    "        df.columns = ['y','x','y_pre','resid']\n",
    "        return df\n",
    "\n",
    "    def R2(self):\n",
    "        y_pre=np.dot(self.X,self.res.params[:2])\n",
    "        SSE=sum((self.y-y_pre)**2)\n",
    "        Mean=self.y.mean()\n",
    "        SST=sum((self.y-Mean)**2)\n",
    "        R2=1-SSE/SST\n",
    "\n",
    "        n=self.y.shape[0]\n",
    "        p=1\n",
    "        adj_R2=1-(1-R2)*(n-1)/(n-p-1)\n",
    "        return R2,adj_R2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from problem2.csv\n",
    "df=pd.read_csv(\"problem2.csv\")\n",
    "y = df.y\n",
    "X = df[['x']].copy()\n",
    "X[\"constant\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model calss\n",
    "mod_norm = Normin(y, X)\n",
    "# condcut fitting\n",
    "res_norm = mod_norm.fit()\n",
    "# get the parameters and corresponding p-value\n",
    "print(res_norm.summary())\n",
    "# calulate the R2, adjusted R2, AIC and BIC\n",
    "R2_norm=mod_norm.R2()\n",
    "print(\"R2 = {} \\nadj_R2 = {}\".format(R2_norm[0],R2_norm[1]))\n",
    "print(\"AIC = {}\".format(res_norm.aic))\n",
    "print(\"BIC = {}\".format(res_norm.bic))\n",
    "\n",
    "\n",
    "# build the model calss\n",
    "mod_T = Tin(y, X)\n",
    "# condcut fitting\n",
    "res_T = mod_T.fit()\n",
    "# get the parameters and corresponding p-value\n",
    "print(res_T.summary())\n",
    "# calulate the R2, adjusted R2, AIC and BIC\n",
    "R2_T=mod_T.R2()\n",
    "print(\"R2 = {} \\nadj_R2 = {}\".format(R2_T[0],R2_T[1]))\n",
    "print(\"AIC = {}\".format(res_T.aic))\n",
    "print(\"BIC = {}\".format(res_T.bic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AR,MA -- statsmodel ARIMA, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def simulate_AR1_process(alpha,beta,sigma,sample_size):\n",
    "    ''' AR(1)\n",
    "        y_t = alpha + beta*y_t-1 + e, e ~ N(0,sigma)\n",
    "    '''\n",
    "    x0=alpha/(1-beta)\n",
    "    x=np.zeros(sample_size+1)\n",
    "    x[0]=x0\n",
    "    eps=stats.norm.rvs(size=sample_size,scale=sigma)\n",
    "    for i in range(sample_size):\n",
    "        x[i+1]=alpha+beta*x[i]+eps[i]\n",
    "    return x\n",
    "\n",
    "def simulate_AR2_process(alpha,beta,sigma,sample_size):\n",
    "    ''' AR(2)\n",
    "        y_t = alpha + beta[0]*y_t-1 + beta[1]*y_t-2 + e, e ~ N(0,sigma)\n",
    "    '''\n",
    "    x0=alpha/(1-sum(beta))\n",
    "    eps=stats.norm.rvs(size=sample_size+1,scale=sigma)\n",
    "    x=np.zeros(sample_size+2)\n",
    "    x[0]=x0\n",
    "    x[1]=x0+eps[0]\n",
    "    for i in range(sample_size):\n",
    "        x[i+2]=alpha+np.dot(beta,x[i:i+2])+eps[i+1]\n",
    "    return x\n",
    "\n",
    "def simulate_AR3_process(alpha,beta,sigma,sample_size):\n",
    "    ''' AR(3)\n",
    "        y_t = alpha + beta[0]*y_t-1 + beta[1]*y_t-2 + beta[2]*y_t-3 + e, e ~ N(0,sigma)\n",
    "    '''\n",
    "    x0=alpha/(1-sum(beta))\n",
    "    eps=stats.norm.rvs(size=sample_size+2,scale=sigma)\n",
    "    x=np.zeros(sample_size+3)\n",
    "    x[0]=x0\n",
    "    x[1]=x[0]+eps[0]\n",
    "    x[2]=x[1]+eps[1]\n",
    "    for i in range(sample_size):\n",
    "        x[i+3]=alpha+np.dot(beta,x[i:i+3])+eps[i+2]\n",
    "    return x\n",
    "\n",
    "\n",
    "def simulate_MA1_process(alpha,beta,sigma,sample_size):\n",
    "    ''' MA(1)\n",
    "        y_t = alpha + e_t + beta*e_t-1, e ~ N(0,sigma)\n",
    "    '''\n",
    "    x0=alpha\n",
    "    x=np.zeros(sample_size+1)\n",
    "    x[0]=x0\n",
    "    eps=stats.norm.rvs(size=sample_size+1,scale=sigma)\n",
    "    for i in range(sample_size):\n",
    "        x[i+1]=alpha+eps[i+1]+beta*eps[i]\n",
    "    return x\n",
    "\n",
    "def simulate_MA2_process(alpha,beta,sigma,sample_size):\n",
    "    ''' MA(2)\n",
    "        y_t = alpha + e_t + beta[0]*e_t-1 + beta[1]*e_t-2, e ~ N(0,sigma)\n",
    "    '''\n",
    "    x0=alpha\n",
    "    eps=stats.norm.rvs(size=sample_size+2,scale=sigma)\n",
    "    x=np.zeros(sample_size+2)\n",
    "    x[0]=x0\n",
    "    x[1]=x0\n",
    "    for i in range(sample_size):\n",
    "        x[i+2]=alpha+eps[i+2]+np.dot(beta,eps[i:i+2][::-1])\n",
    "    return x\n",
    "\n",
    "def simulate_MA3_process(alpha,beta,sigma,sample_size):\n",
    "    ''' MA(3)\n",
    "        y_t = alpha + e_t + beta[0]*e_t-1 + beta[1]*e_t-2 + beta[2]*e_t-3, e ~ N(0,sigma)\n",
    "    '''\n",
    "    x0=alpha\n",
    "    eps=stats.norm.rvs(size=sample_size+3,scale=sigma)\n",
    "    x=np.zeros(sample_size+3)\n",
    "    x[0]=x0\n",
    "    x[1]=x[0]\n",
    "    x[2]=x[0]\n",
    "    for i in range(sample_size):\n",
    "        x[i+3]=alpha+eps[i+3]+np.dot(beta,eps[i:i+3][::-1])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample for AR(1)\n",
    "alpha = 0.75\n",
    "beta = 0.4\n",
    "sigma = 0.8\n",
    "sample_size = 2000\n",
    "ar1_sample=simulate_AR1_process(alpha,beta,sigma,sample_size)\n",
    "\n",
    "# generate sample for AR(2)\n",
    "alpha = 0.75\n",
    "beta = np.array([0.4,0.3])\n",
    "sigma = 0.8\n",
    "sample_size = 2000\n",
    "ar2_sample=simulate_AR2_process(alpha,beta,sigma,sample_size)\n",
    "\n",
    "# generate sample for AR(3)\n",
    "alpha = 0.75\n",
    "beta = np.array([0.2,0.3,0.2])\n",
    "sigma = 0.8\n",
    "sample_size = 2000\n",
    "ar3_sample=simulate_AR3_process(alpha,beta,sigma,sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for AR(1)\n",
    "fig = plt.figure(tight_layout=True,figsize=[10,8])\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "ax = fig.add_subplot(gs[0, :])\n",
    "ax.plot(ar1_sample)\n",
    "ax.set_title(\"AR 1\")\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "sm.graphics.tsa.plot_acf(ar1_sample,ax=ax,lags=30)\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "sm.graphics.tsa.plot_pacf(ar1_sample,method=\"ywm\",ax=ax,lags=30)\n",
    "plt.show()\n",
    "\n",
    "# plot for AR(2)\n",
    "fig = plt.figure(tight_layout=True,figsize=[10,8])\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "ax = fig.add_subplot(gs[0, :])\n",
    "ax.plot(ar2_sample)\n",
    "ax.set_title(\"AR 2\")\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "sm.graphics.tsa.plot_acf(ar2_sample,ax=ax,lags=30)\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "sm.graphics.tsa.plot_pacf(ar2_sample,method=\"ywm\",ax=ax,lags=30)\n",
    "plt.show()\n",
    "\n",
    "# plot for AR(3)\n",
    "fig = plt.figure(tight_layout=True,figsize=[10,8])\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "ax = fig.add_subplot(gs[0, :])\n",
    "ax.plot(ar3_sample)\n",
    "ax.set_title(\"AR 3\")\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "sm.graphics.tsa.plot_acf(ar3_sample,ax=ax,lags=30)\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "sm.graphics.tsa.plot_pacf(ar3_sample,method=\"ywm\",ax=ax,lags=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample for MA(1)\n",
    "alpha = 0.75\n",
    "beta = 0.4\n",
    "sigma = 0.8\n",
    "sample_size = 2000\n",
    "ma1_sample=simulate_MA1_process(alpha,beta,sigma,sample_size)\n",
    "\n",
    "# generate sample for MA(2)\n",
    "alpha = 0.75\n",
    "beta = np.array([0.4,0.3])\n",
    "sigma = 0.8\n",
    "sample_size = 2000\n",
    "ma2_sample=simulate_MA2_process(alpha,beta,sigma,sample_size)\n",
    "\n",
    "# generate sample for MA(3)\n",
    "alpha = 0.75\n",
    "beta = np.array([0.2,0.3,0.2])\n",
    "sigma = 0.8\n",
    "sample_size = 2000\n",
    "ma3_sample=simulate_MA3_process(alpha,beta,sigma,sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for MA(1)\n",
    "fig = plt.figure(tight_layout=True,figsize=[10,8])\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "ax = fig.add_subplot(gs[0, :])\n",
    "ax.plot(ma1_sample)\n",
    "ax.set_title(\"MA 1\")\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "sm.graphics.tsa.plot_acf(ma1_sample,ax=ax,lags=30)\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "sm.graphics.tsa.plot_pacf(ma1_sample,method=\"ywm\",ax=ax,lags=30)\n",
    "plt.show()\n",
    "\n",
    "# plot for MA(2)\n",
    "fig = plt.figure(tight_layout=True,figsize=[10,8])\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "ax = fig.add_subplot(gs[0, :])\n",
    "ax.plot(ma2_sample)\n",
    "ax.set_title(\"MA 2\")\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "sm.graphics.tsa.plot_acf(ma2_sample,ax=ax,lags=30)\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "sm.graphics.tsa.plot_pacf(ma2_sample,method=\"ywm\",ax=ax,lags=30)\n",
    "plt.show()\n",
    "\n",
    "# plot for MA(3)\n",
    "fig = plt.figure(tight_layout=True,figsize=[10,8])\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "ax = fig.add_subplot(gs[0, :])\n",
    "ax.plot(ma3_sample)\n",
    "ax.set_title(\"MA 3\")\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "sm.graphics.tsa.plot_acf(ma3_sample,ax=ax,lags=30)\n",
    "ax = fig.add_subplot(gs[1, 1])\n",
    "sm.graphics.tsa.plot_pacf(ma3_sample,method=\"ywm\",ax=ax,lags=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cholesky Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotPsdError(Exception):\n",
    "    ''' \n",
    "    Used for expection raise if the input matrix is not sysmetric positive definite \n",
    "    '''\n",
    "    pass\n",
    "\n",
    "class chol_psd():\n",
    "    '''\n",
    "    Cholesky Decompstion: Sysmetric Positive Definite matrix could use Cholesky \n",
    "    algorithm to fatorize the matrix to the product between a lower triangle matrix and\n",
    "    upper triangle matrix\n",
    "\n",
    "    Parameter:\n",
    "        matrix  --  Sysmetric Positive Definite (or Positive Semi-definite) \n",
    "                    matrix needed to do Cholesky Factorization.\n",
    "    \n",
    "    Formula: \n",
    "        matrix=L*L.T\n",
    "\n",
    "    Usage:\n",
    "        Chol_model=chol_psd(matrix)\n",
    "        root=Chol_model.root\n",
    "    '''\n",
    "    # initialization\n",
    "    def __init__(self,matrix):\n",
    "        self.__psd=matrix\n",
    "        self.run()\n",
    "\n",
    "    # Perform the Cholesky Factorization\n",
    "    def run(self):\n",
    "        n=self.__psd.shape[0]\n",
    "        root=np.zeros([n,n])\n",
    "        for i in range(n):\n",
    "            # diagonal\n",
    "            root[i][i] = self.__psd[i][i] - root[i][:i] @ root[i][:i].T\n",
    "            root[i][i]=0 if 0>=root[i][i]>=-1e-8 else root[i][i]\n",
    "            # if the diagonal element is less than -1e-8, it might not be PSD\n",
    "            if root[i][i]<0:\n",
    "                raise NotPsdError(\"Not PSD!\")\n",
    "            root[i][i]=np.sqrt(root[i][i])\n",
    "            \n",
    "            #below the diagonal\n",
    "            # if diagonal element is zero, set the following element of that column to be zero too\n",
    "            if root[i][i]==0:\n",
    "                continue\n",
    "            for j in range(i+1,n):\n",
    "                root[j][i]=(self.__psd[j][i]-root[i,:i] @ root[j,:i])/root[i][i]\n",
    "        self.__root=root\n",
    "        self.__ispsd=True\n",
    "\n",
    "    @property\n",
    "    def root(self):\n",
    "        return self.__root   \n",
    "    \n",
    "    @property\n",
    "    def ispsd(self):\n",
    "        return self.__ispsd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# PD\n",
    "sigma=np.full([5,5],0.9)\n",
    "np.fill_diagonal(sigma, 1)\n",
    "root=chol_psd(sigma).root\n",
    "print(np.allclose(root@root.T,sigma))\n",
    "\n",
    "# PSD\n",
    "sigma[0][1]=1\n",
    "sigma[1][0]=1\n",
    "v,c=np.linalg.eig(sigma)\n",
    "root=chol_psd(sigma).root\n",
    "print(np.allclose(root@root.T,sigma))\n",
    "\n",
    "# not PSD\n",
    "sigma[0][1]=0.7357\n",
    "sigma[1][0]=0.7357\n",
    "root=chol_psd(sigma).root\n",
    "print(np.allclose(root@root.T,sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cholesky Factorization to test whether a matrix is PSD or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotSysmetricError(Exception):\n",
    "    ''' \n",
    "    Used for expection raise if the input matrix is not sysmetric\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "class NegativeEigError(Exception):\n",
    "    ''' \n",
    "    Used for expection raise if matrix has the negative eigvalue\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "class PSD:\n",
    "    \"\"\"\n",
    "    PSD class is used for Positive Semi-Definite Matrix confirmation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def confirm(psd):\n",
    "        # make sure sysmetric\n",
    "        if not np.allclose(psd,psd.T):\n",
    "            raise NotSysmetricError(\"Matrix does not equal to Matrix.T\")\n",
    "        # Make sure no negative eigenvalues\n",
    "        eig_val=np.linalg.eigvals(psd)\n",
    "        neg_eig=len(eig_val[eig_val<0])\n",
    "        # No negative eigenvalues or Pass the Cholesky algorithm\n",
    "        if neg_eig==0 or chol_psd(psd).ispsd:\n",
    "            print(\"Matrix is Sysmetric Positive Definite!\")\n",
    "            return True\n",
    "        else:\n",
    "            raise NegativeEigError(\"Matrix has negative eigenvalue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSD.confirm(RJ_psd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentially Weighted Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Exponentially Weighted Moving Average for Volatility Model -- Exponentially Weighted Covariance\n",
    "class EWMA:\n",
    "    \"\"\"\n",
    "    Calculate the Exponentially Weighted Covariance & Correaltion Matrix\n",
    "    \n",
    "    Parameter: \n",
    "        data (np.array)  -- return data for calculating Covariance & Correaltion Matrix (array)\n",
    "        lambda_  -- smoothing parameter (less than 1)\n",
    "        flag (optional)  -- a flag to dertermine whether to subtract mean from data.\n",
    "                            if it set False, data would not subtract its mean.\n",
    "\n",
    "    fomula: \\sigma_t^2=\\lambda \\sigma_{t-1}^2+(1-\\lambda)r_{t-1}^2\n",
    "\n",
    "    Usage:  \n",
    "        model=EWMA(data,0.97)\n",
    "        cov_mat=model.cov_mat\n",
    "        corr_mat=model.corr_mat\n",
    "    \"\"\"\n",
    "    # initialization \n",
    "    def __init__(self,data,lambda_,flag=False):\n",
    "        self.__data=data if flag==False else data-data.mean(axis=0)\n",
    "        self.__lambda=lambda_\n",
    "        self.get_weight() \n",
    "        self.cov_matrix()\n",
    "        self.corr_matrix()\n",
    "\n",
    "    # calculate the weight matrix\n",
    "    def get_weight(self):\n",
    "        n=self.__data.shape[0]\n",
    "        weight_mat=[(1-self.__lambda)*self.__lambda**(n-i-1) for i in range(n)]\n",
    "        weight_mat=weight_mat/sum(weight_mat)\n",
    "        self.__weight_mat=np.diag(weight_mat)\n",
    "\n",
    "    # calculate cov_matrix\n",
    "    def cov_matrix(self):\n",
    "        self.__cov_mat=self.__data.T @ self.__weight_mat @ self.__data\n",
    "\n",
    "    # calculate corr_matrix\n",
    "    def corr_matrix(self):\n",
    "        n=self.__data.shape[1]\n",
    "        invSD=np.sqrt(1./np.diag(self.__cov_mat))\n",
    "        invSD=np.diag(invSD)\n",
    "        self.__corr_mat=invSD @ self.__cov_mat @ invSD\n",
    "        return self.__corr_mat\n",
    "\n",
    "    # plot the cumulative weight\n",
    "    def plot_weight(self,k=None,ax=None,label=None):\n",
    "        weight=np.diag(self.__weight_mat)[::-1]\n",
    "        cum_weight=weight.cumsum()/weight.sum()\n",
    "        sns.lineplot(cum_weight,ax=ax,label=\"{:.2f}\".format(label) if label!=None else \"\")\n",
    "        if ax!=None:\n",
    "            ax.set_xlabel('Time Lags')\n",
    "            ax.set_ylabel('Cumulative Weights')\n",
    "            ax.set_title(\"Weights of differnent lambda\")\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "    @property\n",
    "    def cov_mat(self):\n",
    "        return self.__cov_mat    \n",
    "\n",
    "    @property\n",
    "    def corr_mat(self):\n",
    "        return self.__corr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the .csv file into dataframe\n",
    "df=pd.read_csv('DailyReturn.csv',index_col=0)\n",
    "# transfrom dataframe to array to speed up matrix calculation\n",
    "data=np.array(df)\n",
    "# EWMA\n",
    "lambdaValue=0.97\n",
    "model=EWMA(data,lambdaValue) \n",
    "cov_mat=model.cov_mat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Non-PSD Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Non_psd_mat:\n",
    "    \"\"\"\n",
    "    Used to generate the non-positive semi-definite matrix\n",
    "    \"\"\"\n",
    "    def non_psd_mat(self,n):\n",
    "        corr = np.full((n, n), 0.9)\n",
    "        np.fill_diagonal(corr, 1)\n",
    "        corr[0, 1] = 0.7357  \n",
    "        corr[1, 0] = 0.7357 \n",
    "        return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_psd_mat(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Frobenius Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weighted_F_norm:\n",
    "    '''\n",
    "    Given the weight matrix, calculate the Weighted Frobenius Norm. (Assume it's diagonal)\n",
    "    '''\n",
    "    def compare_F(self,mat_a,mat_b,mat_w):\n",
    "        '''Give two matrix, use Weighted Frobenius Norm to calculate how close they are'''\n",
    "        err = mat_a-mat_b #difference\n",
    "        weighted_err = np.sqrt(mat_w) @ err @ np.sqrt(mat_w) \n",
    "        w_F_norm = np.sqrt(np.square(weighted_err).sum())\n",
    "        return w_F_norm\n",
    "    \n",
    "    def calculate_F(self,mat,mat_w):\n",
    "        \"Given one matrix, calculate its Weighted Frobenius Norm\"\n",
    "        weighted_err = np.sqrt(mat_w) @ mat @ np.sqrt(mat_w)\n",
    "        w_F_norm = np.sqrt(np.square(weighted_err).sum())\n",
    "        return w_F_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebonato and Jackel's Method (Non-PSD -> PSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class near_psd(Weighted_F_norm):\n",
    "    '''\n",
    "    Rebonato and Jackel's Method to get acceptable PSD matrix \n",
    "    \n",
    "    Parameters:\n",
    "        not_psd -- the matrix which is not positive semi-definite matrix\n",
    "        weight  -- used for calculating the Weighted Frobenius Norm (Now assume it's diagonal)\n",
    "\n",
    "    Usage:\n",
    "        near_psd_model=near_psd(non_psd,weight)\n",
    "        psd=near_psd_model.psd\n",
    "    '''\n",
    "    # initialization\n",
    "    def __init__(self,not_psd,weight):\n",
    "        self.__not_psd=not_psd\n",
    "        self.__weight=weight\n",
    "        self.run() # main function\n",
    "        self.F_compare_norm(weight) # Weighted Frobenius Norm\n",
    "        \n",
    "    def run(self):\n",
    "        n=self.__not_psd.shape[0]\n",
    "        # Set the weight matrix to be identity matrix\n",
    "        invSD = np.eye(n)\n",
    "        corr=self.__not_psd\n",
    "        # if the matrix is not correlation matrix, convert it to the correlation matrix\n",
    "        if not np.allclose(np.diag(self.__not_psd),np.ones(n)):\n",
    "            invSD=np.diag(1/np.sqrt(np.diag(self.__not_psd)))\n",
    "            corr=invSD @ self.__not_psd @ invSD\n",
    "        eig_val,eig_vec=np.linalg.eigh(corr) # eigenvalues & eigenvectors \n",
    "        eig_val[eig_val<0]=0 # adjust the negative value to 0\n",
    "        # get the scale matrix\n",
    "        scale_mat = np.diag(1/((eig_vec * eig_vec) @ eig_val))\n",
    "        B = np.sqrt(scale_mat) @ eig_vec @ np.sqrt(np.diag(eig_val))\n",
    "        corr=B @ B.T\n",
    "        # convert it back into original form\n",
    "        SD=np.diag(1/np.diag(invSD))\n",
    "        psd = SD @ corr @ SD\n",
    "        self.__psd = psd\n",
    "\n",
    "    # Weighted Frobenius Norm of the difference between near_psd and ono_psd\n",
    "    def F_compare_norm(self,weight):\n",
    "        self.__F = self.compare_F(self.__psd,self.__not_psd,weight)\n",
    "\n",
    "    @property\n",
    "    def psd(self):\n",
    "        return self.__psd\n",
    "    \n",
    "    @property\n",
    "    def F(self):\n",
    "        return self.__F\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = non_psd_mat(10) # np.array\n",
    "weight=np.eye(10)\n",
    "near_psd(mat,weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higham's Method to find nearest PSD correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Higham_psd(Weighted_F_norm,chol_psd):\n",
    "    '''\n",
    "    Higham's Method to get nearest PSD matrix under the measure of Weighted Frobenius Norm\n",
    "    Now Assume the Weighted Matrix is diagonal\n",
    "    \n",
    "    Parameters:\n",
    "        not_psd -- the matrix which is not positive semi-definite matrix\n",
    "        weight  -- used for calculating the Weighted Frobenius Norm (Assume it's diagonal)\n",
    "        epsilon -- the acceptable precision between near_psd and non_psd\n",
    "        max_iter -- maximum iteration number\n",
    "\n",
    "    Usage:\n",
    "        Higham_psd_model=Higham_psd(non_psd,weight)\n",
    "        psd=Higham_psd_model.psd\n",
    "    '''\n",
    "    # initialization\n",
    "    def __init__(self,not_psd,weight,epsilon=1e-9,max_iter=1e10):\n",
    "        self.__not_psd=not_psd\n",
    "        self.__weight=weight\n",
    "        self.run(epsilon=epsilon,max_iter=max_iter)\n",
    "        self.F_compare_norm(weight)\n",
    "\n",
    "    def Projection_U(self,A):\n",
    "        # Projection to the Space U\n",
    "        # we assume that the weight matrix is diagonal\n",
    "        A_copy=A.copy()\n",
    "        np.fill_diagonal(A_copy,1)\n",
    "        return A_copy\n",
    "        \n",
    "    def Projection_S(self,A):\n",
    "        # Projection to the Space S\n",
    "        w_sqrt=np.sqrt(self.__weight)\n",
    "        eig_val,eig_vec=np.linalg.eigh(w_sqrt @ A @ w_sqrt)\n",
    "        eig_val[eig_val<0]=0\n",
    "        A_plus=eig_vec @ np.diag(eig_val) @ eig_vec.T\n",
    "        w_sqrt_inv=np.diag(1/np.diag(w_sqrt))\n",
    "        ans = w_sqrt_inv @ A_plus @ w_sqrt_inv\n",
    "        return ans\n",
    "    \n",
    "    def run(self,epsilon,max_iter):\n",
    "        # iterating process\n",
    "        Y=self.__not_psd\n",
    "        F1=np.inf\n",
    "        F2=self.calculate_F(Y,self.__weight)\n",
    "        delta=0\n",
    "        iteration=0\n",
    "        neg_eig=0\n",
    "        while abs(F1-F2)>epsilon or neg_eig>0:\n",
    "            R=Y-delta\n",
    "            X=self.Projection_S(R)\n",
    "            delta=X-R\n",
    "            Y=self.Projection_U(X)\n",
    "            F1,F2=F2,self.calculate_F(Y,self.__weight)\n",
    "            iteration+=1\n",
    "            if iteration>max_iter:\n",
    "                break\n",
    "            eig_val=np.linalg.eigvals(Y)\n",
    "            neg_eig=len(eig_val[eig_val<0])\n",
    "\n",
    "        self.__F_norm=F2\n",
    "        self.__psd=Y\n",
    "\n",
    "    def F_compare_norm(self,weight):\n",
    "        self.__F = self.compare_F(self.__psd,self.__not_psd,weight)\n",
    "        \n",
    "    @property\n",
    "    def psd(self):\n",
    "        return self.__psd \n",
    "    @property\n",
    "    def F_norm(self):\n",
    "        return self.__F_norm \n",
    "    \n",
    "    @property\n",
    "    def F(self):\n",
    "        return self.__F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = non_psd_mat(10) # np.array\n",
    "weight=np.eye(10)\n",
    "Higham_psd(mat,weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    \"\"\"\n",
    "    Reducing the dimensionality of the dataset.\n",
    "    \n",
    "    Parameter:\n",
    "        cov_mat -- covarinance matrix for dimensionality reduction\n",
    "        threshold(optional) -- the threshold of cumulative variance explained\n",
    "    \n",
    "    Usage:\n",
    "        PCA_model=PCA(cov_mat,threshold)\n",
    "        PCA_model.plot()\n",
    "        PCA_model.pct_explain_val_vec()\n",
    "    \"\"\"\n",
    "    # initialization\n",
    "    def __init__(self,cov_mat,threshold=None):\n",
    "        self.__cov_mat=cov_mat\n",
    "        self.__threshold=threshold\n",
    "        self.run()\n",
    "    \n",
    "    # Conduct the PCA\n",
    "    def run(self):\n",
    "        self.__eig_val,self.__eig_vec = np.linalg.eigh(self.__cov_mat)\n",
    "        # pick the eigenvalues which is bigger than 0 and corresponding eigenvector   \n",
    "        idx=self.__eig_val>1e-8\n",
    "        self.__eig_val = self.__eig_val[idx]\n",
    "        self.__eig_vec = self.__eig_vec [:,idx]       \n",
    "        # sort since the result given by numpy is lowest to highest, flip them up\n",
    "        sorted_indices = np.argsort(self.__eig_val)\n",
    "        self.__eig_val=self.__eig_val[sorted_indices[::-1]]\n",
    "        self.__eig_vec=self.__eig_vec[:,sorted_indices[::-1]]\n",
    "        \n",
    "\n",
    "    # calculate the cumulative percent of variance explained\n",
    "    def percent_expalined(self,k=None):\n",
    "        k = self.__eig_val.shape[0] if k==None else k\n",
    "        k_eig_val=self.__eig_val[:k]\n",
    "\n",
    "        return k_eig_val.cumsum()/k_eig_val.sum()\n",
    "\n",
    "    # plot the cumulative percent of variance explained\n",
    "    def plot(self,k=None,ax=None,label=None):\n",
    "        explain=self.percent_expalined(k)\n",
    "        sns.lineplot(explain,ax=ax,label=\"{:.2f}\".format(label) if label!=None else \"\" )\n",
    "        if ax!=None:\n",
    "            ax.set_xlabel('Number of Component')\n",
    "            ax.set_ylabel('Cumulative Variance Explained')\n",
    "            ax.set_title(\"Cumulative Variance Explained via different lambda\")\n",
    "        ax.legend(loc='best')\n",
    "    \n",
    "    # Given the threshold, calculate the needed eigenvalues and corresponding eigenvectors\n",
    "    def pct_explain_val_vec(self):\n",
    "        eig_val=self.__eig_val\n",
    "        pct_cum_var=eig_val.cumsum()/eig_val.sum()\n",
    "        pct_cum_var[-1]=1\n",
    "        # if cumulative percent of variance explained is larger than threshold, then break\n",
    "        k = bisect.bisect_left(pct_cum_var, self.__threshold) \n",
    "        return self.__eig_val[:k+1],self.__eig_vec[:,:k+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the .csv file into dataframe\n",
    "df=pd.read_csv('DailyReturn.csv',index_col=0)\n",
    "# transfrom dataframe to array to speed up matrix calculation\n",
    "data=np.array(df)\n",
    "# PCA -- Use PCA and plot the cumulative variance explained by each eigenvalue for λ ∈ (0, 1) each chosen.\n",
    "PCA_model=PCA(cov_mat) \n",
    "PCA_model.plot(ax=ax[0],label=i) \n",
    "model.plot_weight(ax=ax[1],label=i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulator for Direct Simulation & PCA_Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator(Weighted_F_norm):\n",
    "    \"\"\"\n",
    "    Simulator for DirectSimulation & PCA_Simulation\n",
    "\n",
    "    Parameter: \n",
    "        cov_mat -- covariance matrix\n",
    "        draw_num -- the number of sample draw from simulation\n",
    "\n",
    "    Usage:\n",
    "        simulator = Simulator(cov_mat,draw_num)\n",
    "        simulator.DirectSimulation()\n",
    "        simulator.PCA_Simulation(pct)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,cov_mat,draw_num):\n",
    "        self.__cov_mat=cov_mat\n",
    "        self.__draw_num=draw_num\n",
    "        self.__Direct_run_time=None\n",
    "        self.__PCA_run_time=None\n",
    "        self.__X=None # generated sample\n",
    "\n",
    "    def DirectSimulation(self):\n",
    "        \"\"\"Cholesky\"\"\"\n",
    "        t = time.time()\n",
    "        root=chol_psd(self.__cov_mat).root\n",
    "        n=root.shape[0]\n",
    "        rand_norm=np.random.normal(0, 1, size=(n, self.__draw_num))\n",
    "        X= root @ rand_norm\n",
    "        self.__Direct_run_time = time.time()-t\n",
    "        self.__X=X #simulated sample\n",
    "        return X \n",
    "\n",
    "    def PCA_Simulation(self,threshold):\n",
    "        \"\"\"PCA\"\"\"\n",
    "        t = time.time()\n",
    "        PCA_model=PCA(self.__cov_mat,threshold=threshold)\n",
    "        eig_val,eig_vec=PCA_model.pct_explain_val_vec()\n",
    "        B= eig_vec @ np.diag(np.sqrt(eig_val))\n",
    "        n=B.shape[1]\n",
    "        rand_norm=np.random.normal(0, 1, size=(n, self.__draw_num))\n",
    "        X= B @ rand_norm\n",
    "        self.__PCA_run_time = time.time()-t\n",
    "        self.__X=X #simulated sample\n",
    "        return X\n",
    "\n",
    "    # get the F norm of difference between covariance of simulation and original covariance\n",
    "    def err_F_norm(self):\n",
    "        n=self.__cov_mat.shape[0]\n",
    "        w=np.eye(n)\n",
    "        return self.compare_F(self.__cov_mat,np.cov(self.__X),w)\n",
    "     \n",
    "    @property\n",
    "    def Direct_run_time(self):\n",
    "        return self.__Direct_run_time\n",
    "\n",
    "    @property\n",
    "    def PCA_run_time(self):\n",
    "        return self.__PCA_run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = Simulator(cov_mat,draw_num)\n",
    "sample1=simulator.DirectSimulation()\n",
    "sample2=simulator.PCA_Simulation(pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_calculate(Price,option=\"DISCRETE\",rm_means=True):\n",
    "    '''\n",
    "        Provide two ways to calculate the return from Price dataframce.\n",
    "        The date should be accumulative(e.g. 10 Oct [1st row], 11 Oct [2nd row]....)\n",
    "    '''\n",
    "    # calculate the log normal return \n",
    "    if option == 'CONTINUOUS':\n",
    "        returns = np.log(df/df.shift()).dropna()\n",
    "    # calculate the discrete return \n",
    "    elif  option == 'DISCRETE':\n",
    "        returns = df.pct_change().dropna()\n",
    "    # other undefined option will cause error\n",
    "    else:\n",
    "        raise Exception(\"Unknown Option!\")\n",
    "    # remove mean from the returns\n",
    "    return returns if rm_means==False else returns-returns.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized T distribution with mean (MLE Fitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T_fitter:\n",
    "    '''T distribution MEL fitter'''\n",
    "    def ll_t(self,parameter,x):\n",
    "        # log likelihood \n",
    "        ll=np.sum(stats.t.logpdf(x=x,df=parameter[0],loc=0,scale=parameter[1])) # assume mean to be 0\n",
    "        return -ll\n",
    "\n",
    "    def MLE(self,x):\n",
    "        cons=[ {'type':'ineq', 'fun':lambda x:x[1]} ] # standard deviation is non-negative\n",
    "        parameter = np.array([x.size-1,2])\n",
    "        MLE = minimize(self.ll_t, parameter, args = x, constraints = cons) # MLE\n",
    "        return MLE.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Asset VaR under different distribution (Normal, T, AR(1), Historical) --  需要修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VaR:\n",
    "    '''Calculate the VaR of 1D array or dataframe of return due to specific distribution'''\n",
    "    def __init__(self,data,option=\"Absolute\",alpha=0.05):\n",
    "        # Absolute Value at risk or Relative Value at risk\n",
    "        if option != \"Absolute\" and option != \"Relative\":\n",
    "            raise Exception('Unknown option!')\n",
    "        self.__option=option\n",
    "        # 1-alpha is confidence level\n",
    "        self.__alpha=alpha\n",
    "        # returns data (DataFrame)\n",
    "        self.__data=data\n",
    "\n",
    "    def normal(self,option='Normal',plot=False):\n",
    "        '''Assume returns follow normal distribution'''\n",
    "        # assume mean to be 0\n",
    "        mu=0\n",
    "        # calculate the standard deviation\n",
    "        if option == 'Normal': # use normal standard deviation\n",
    "            std=self.__data.std() \n",
    "        elif option == 'EWMA': # use EW standard deviation\n",
    "            model=project3.EWMA(META,0.94) # assume lambda = 0.94\n",
    "            std=np.sqrt(model.cov_mat)\n",
    "        else: \n",
    "            raise Exception('Unknown option!')\n",
    "         # calculate the VaR\n",
    "        if self.__option==\"Absolute\":\n",
    "            VaR=-stats.norm.ppf(self.__alpha,loc=mu,scale=std)\n",
    "        else:\n",
    "            VaR=self.__data.mean()-stats.norm.ppf(self.__alpha,loc=mu,scale=std)\n",
    "\n",
    "        if plot:\n",
    "            # plot the normal Probability density function\n",
    "            max_val=self.__data.max()\n",
    "            min_val=self.__data.min()\n",
    "            x=np.linspace(min_val,max_val,1000)\n",
    "            y=stats.norm.pdf(x=x,loc=mu,scale=std)\n",
    "            plt.plot(x,y,color='brown')\n",
    "            # fill VaR area\n",
    "            plt.fill_between(x,y,where=x<-VaR,color=\"red\", alpha=0.3)\n",
    "            # plot the return data & its empirical kde\n",
    "            sns.histplot(self.__data,kde=True,stat='density')\n",
    "            # plot the VaR\n",
    "            plt.axvline(-VaR,color='#FF6347')\n",
    "            if option == 'Normal':\n",
    "                plt.title(\"Normal\")\n",
    "                plt.legend(['Normal','VaR','historical'])\n",
    "            else:\n",
    "                plt.title(\"Normal with EW standard deviation\")\n",
    "                plt.legend(['Normal','VaR','historical'])\n",
    "        return VaR\n",
    "        \n",
    "    def T_dist(self,plot=False):\n",
    "        para=T_fitter().MLE(self.__data)\n",
    "        mu=0 # assume mean to be 0\n",
    "        df=para[0] # degree of freedom of T distribution\n",
    "        std=para[1] # standard deviation\n",
    "        # calculate the VaR\n",
    "        if self.__option==\"Absolute\":\n",
    "            VaR=-stats.t.ppf(self.__alpha,df=df,loc=mu,scale=std)\n",
    "        else:\n",
    "            VaR=self.__data.mean()-stats.t.ppf(self.__alpha,df=df,loc=mu,scale=std)\n",
    "        if plot:\n",
    "            # plot the T Probability density function\n",
    "            max_val=self.__data.max()\n",
    "            min_val=self.__data.min()\n",
    "            x=np.linspace(min_val,max_val,1000)\n",
    "            y=stats.t.pdf(x=x,df=df,loc=mu,scale=std)\n",
    "            plt.plot(x,y,color='brown')\n",
    "            # fill VaR area\n",
    "            plt.fill_between(x,y,where=x<-VaR,color=\"red\", alpha=0.3)\n",
    "            # plot the return data & its empirical kde\n",
    "            sns.histplot(self.__data,kde=True,stat='density')\n",
    "            # plot the VaR\n",
    "            plt.axvline(-VaR,color='#FF6347')\n",
    "            plt.title(\"MLE fitted T distribution\")\n",
    "            plt.legend(['T distribution','VaR','historical'])\n",
    "        return VaR\n",
    "\n",
    "    def AR_1(self,plot=False):\n",
    "        # Use AR(1) fitter to find the best cofficience\n",
    "        mod = sm.tsa.arima.ARIMA(self.__data, order=(1, 0, 0))\n",
    "        res = mod.fit()\n",
    "        const=0 # constant number\n",
    "        ar_L1=res.params[1] # cofficience of Lag 1\n",
    "        sigma2=res.params[2] # variance of error term\n",
    "\n",
    "        # AR(1) is also normal\n",
    "        mu=0\n",
    "        std=np.sqrt(sigma2/(1-ar_L1))\n",
    "\n",
    "        # calculate the VaR\n",
    "        if self.__option==\"Absolute\":\n",
    "            VaR=-stats.norm.ppf(self.__alpha,loc=mu,scale=std)\n",
    "        else:\n",
    "            VaR=self.__data.mean()-stats.norm.ppf(self.__alpha,loc=mu,scale=std)\n",
    "        \n",
    "        if plot:\n",
    "            # plot the AR(1) Probability density function\n",
    "            max_val=self.__data.max()\n",
    "            min_val=self.__data.min()\n",
    "            x=np.linspace(min_val,max_val,1000)\n",
    "            y=stats.norm.pdf(x=x,loc=mu,scale=std)\n",
    "            plt.plot(x,y,color='brown')\n",
    "            # fill VaR area\n",
    "            plt.fill_between(x,y,where=x<-VaR,color=\"red\", alpha=0.3)\n",
    "            # plot the return data & its empirical kde\n",
    "            sns.histplot(self.__data,kde=True,stat='density')\n",
    "            # plot the VaR\n",
    "            plt.axvline(-VaR,color='#FF6347')\n",
    "            plt.title(\"fitted AR(1)\")\n",
    "            plt.legend(['fitted AR(1)','VaR','historical'])\n",
    "        return VaR\n",
    "    \n",
    "    def historical_simulation(self,alpha,plot=False):\n",
    "        size=self.__data.shape[0]\n",
    "        rt=self.__data.sample(n=size,replace=True)\n",
    "        # calculate the VaR\n",
    "        if self.__option==\"Absolute\":\n",
    "            VaR=-np.quantile(rt,alpha)\n",
    "        else:\n",
    "            VaR=rt.mean()-np.quantile(rt,alpha)\n",
    "        if plot:\n",
    "            # plot the historical kde\n",
    "            sns.kdeplot(rt,color='brown')\n",
    "\n",
    "            # plot the return data & its empirical kde\n",
    "            ax = sns.histplot(self.__data,kde=True,stat='density')\n",
    "            \n",
    "            # fill VaR area\n",
    "            # Get the two lines from the axes to generate shading\n",
    "            l = ax.lines[0]\n",
    "            # Get the xy data from the lines so that we can shade\n",
    "            x = l.get_xydata()[:,0]\n",
    "            y = l.get_xydata()[:,1]\n",
    "            ax.fill_between(x,y,where=x<-VaR,color=\"red\", alpha=0.3)\n",
    "\n",
    "            # plot the VaR\n",
    "            plt.axvline(-VaR,color='#FF6347')\n",
    "            plt.title(\"historical simulation\")\n",
    "            plt.legend(['historical simulation','VaR','historical'])\n",
    "        return VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the DailyPrices data into dataframe\n",
    "df=pd.read_csv('DailyPrices.csv',index_col='Date')\n",
    "# calculate the returns\n",
    "rt=return_calculate(df)\n",
    "META=rt['META']\n",
    "\n",
    "# Using a normal distribution\n",
    "VaR_normal=VaR(META).normal(plot=True)\n",
    "print(\"VaR (Normal Distribution): {}\".format(VaR_normal))\n",
    "# Using a normal distribution with an Exponentially Weighted variance \n",
    "VaR_normal_EWMA=VaR(META).normal('EWMA',plot=True)\n",
    "print(\"VaR (Normal Distribution + EWMA): {}\".format(VaR_normal_EWMA))\n",
    "# Using a MLE fitted T distribution\n",
    "VaR_T=VaR(META).T_dist(plot=True)\n",
    "print(\"VaR (MLE Fitted T Distribution): {}\".format(VaR_T))\n",
    "# Using a fitted AR(1) model.\n",
    "VaR_AR_1=VaR(META).AR_1(plot=True)\n",
    "print(\"VaR (AR(1) Process): {}\".format(VaR_AR_1))\n",
    "# Using a Historic Simulation.\n",
    "VaR_normal_historical=VaR(META).historical_simulation(0.05,plot=True)\n",
    "print(\"VaR (Historical sampling): {}\".format(VaR_normal_historical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Var (Delta Normal, Normal MC, Historical Simulation) -- 也要改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VaR_portfolio:\n",
    "    '''Differnet Method to get the VaR of stock portfolio'''\n",
    "    # initialization\n",
    "    def __init__(self,portfolio,returns,price):\n",
    "        '''The fromat of data should be same as the file in the current directory.'''\n",
    "        # information about portfolio\n",
    "        self.__portfolio=portfolio\n",
    "        # returns information about companies\n",
    "        self.__returns=returns\n",
    "        # price about stock\n",
    "        self.__price=price\n",
    "\n",
    "    def delta_normal(self,alpha=0.05,option='EWMA'):\n",
    "        ''' Delta Normal method to calculate the VaR of the Stock portfolio\n",
    "            \n",
    "            Parmeter:\n",
    "                option='EWMA' or 'std'\n",
    "        '''\n",
    "        stocks = self.__portfolio.Stock.values # stocks of portfilio\n",
    "        rt=self.__returns[stocks] # return of each stock of the portfilio\n",
    "        portfolio=self.__portfolio.set_index('Stock') # set the portfolio index to be Stock\n",
    "        holding=portfolio.loc[stocks].Holding # get holding of each stock of the portfilio\n",
    "        current_price=self.__price[stocks].iloc[-1,:] # get current price of each stock of the portfilio\n",
    "        current_position = holding * current_price # current position of each stock of the portfilio\n",
    "        PV = current_position.sum() # portfolio value\n",
    "        delta=current_position/PV # calculate the delta\n",
    "        stocks = self.__portfolio.Stock.values\n",
    "        rt=self.__returns[stocks]\n",
    "\n",
    "        if option=='EWMA':\n",
    "            # Calculate EWMA covariance\n",
    "            model=project3.EWMA(rt.values,0.94) # assume lambda = 0.94\n",
    "            cov_mat=model.cov_mat\n",
    "        elif option=='std':\n",
    "            # Calculate standard deviation\n",
    "            cov_mat=rt.cov()\n",
    "\n",
    "        # calculate the std of portfolio\n",
    "        std = np.sqrt(delta @ cov_mat @ delta)\n",
    "        # get the VaR of the portfolio\n",
    "        VaR_p = PV*(-stats.norm.ppf(alpha))*std\n",
    "        return VaR_p\n",
    "\n",
    "    def normal_MC(self,alpha=0.05,option='EWMA',method='PCA',draw_num=100000,pct=1,plot=False,VaROption='Absolute',p_name=''):\n",
    "        ''' Use Monte Carlo Methods to simulate the price of each stock of portfolio\n",
    "            then calculate the value of the portfolio\n",
    "            \n",
    "            Parmeter:\n",
    "                option: 'EWMA' or 'std'\n",
    "                method: 'PCA' or 'Cholesky'\n",
    "                VaROption: 'Absolute' VaR or 'Relative' VaR\n",
    "                draw_num: number of path simulated\n",
    "                pct: the percentage of variance expained by PCA\n",
    "                plot: plot the simulated path or not\n",
    "                alpha: 1-alpha is confidence level\n",
    "                p_name: portfolio name\n",
    "        '''\n",
    "        stocks = self.__portfolio.Stock.values # stocks of portfilio\n",
    "        rt=self.__returns[stocks] # return of each stock of the portfilio\n",
    "        portfolio=self.__portfolio.set_index('Stock') # set the portfolio index to be Stock\n",
    "        holding=portfolio.loc[stocks].Holding # get holding of each stock of the portfilio\n",
    "        current_price=self.__price[stocks].iloc[-1,:] # get current price of each stock of the portfilio\n",
    "\n",
    "        # get the covariance\n",
    "        if option=='EWMA':\n",
    "            # Calculate EWMA covariance\n",
    "            model=project3.EWMA(rt.values,0.94) # assume lambda = 0.94\n",
    "            cov_mat=model.cov_mat\n",
    "        elif option=='std':\n",
    "            # Calculate standard deviation\n",
    "            cov_mat=rt.cov()\n",
    "        else:\n",
    "            raise Exception(\"Unknown option!\")\n",
    "        \n",
    "        # MC to get the simulated return\n",
    "        simulator = project3.Simulator(cov_mat,draw_num)\n",
    "        if method=='PCA':\n",
    "            simulated_rt=simulator.PCA_Simulation(pct)\n",
    "        elif method=='Cholesky':\n",
    "            simulated_rt=simulator.DirectSimulation()\n",
    "        else:\n",
    "            raise Exception(\"Unknown method!\")\n",
    "        # simulated price\n",
    "        simulate_price = np.expand_dims(current_price,1).repeat(draw_num,axis=1) * simulated_rt\n",
    "        # simulated position\n",
    "        simulate_position=np.expand_dims(holding,1).repeat(draw_num,axis=1) * simulate_price\n",
    "        # simulated portfolio value\n",
    "        simulate_PV=pd.DataFrame(simulate_position).sum()\n",
    "        # sort\n",
    "        simulate_PV=pd.DataFrame(simulate_PV.sort_values(ascending=True))\n",
    "    \n",
    "        if VaROption==\"Absolute\":\n",
    "            VaR_p=-np.quantile(simulate_PV,alpha)\n",
    "        elif VaROption=='Relative':\n",
    "            VaR_p=simulate_PV.mean()-np.quantile(simulate_PV,alpha)\n",
    "        else:\n",
    "            raise Exception(\"Unknown VaROption!\")\n",
    "        \n",
    "        \n",
    "        if plot:\n",
    "            # add current Portfolio value\n",
    "            simulate_PV[1]=simulate_PV\n",
    "            simulate_PV[0]=0\n",
    "            plot_data=simulate_PV.T\n",
    "            # plot\n",
    "            fig, ax = plt.subplots(1,2,figsize=(14,6))\n",
    "            plot_data.plot(ax=ax[0],legend=False,xlabel='Time',ylabel='Price',title=\"Mote Carlo Simulation({} path) for portfolio {}\".format(draw_num,p_name))\n",
    "            sns.histplot(data=simulate_PV[1],kde=True,stat=\"density\",ax=ax[1])\n",
    "\n",
    "            # fill VaR area\n",
    "            # Get the two lines from the axes to generate shading\n",
    "            l = ax[1].lines[0]\n",
    "            # Get the xy data from the lines so that we can shade\n",
    "            x = l.get_xydata()[:,0]\n",
    "            y = l.get_xydata()[:,1]\n",
    "            ax[1].fill_between(x,y,where=x<-VaR_p,color=\"red\", alpha=0.3)\n",
    "            \n",
    "            # plot the VaR\n",
    "            plt.axvline(-VaR_p,color='#FF6347')\n",
    "            plt.title(\"Monte Carlo Simulated VaR($) of Portfolio {}\".format(p_name))\n",
    "            plt.legend(['MC simulation kde','VaR'])\n",
    "        return VaR_p\n",
    "\n",
    "    def historical_simulation(self,alpha=0.05,draw_num=10000,plot=False,p_name='',VaROption='Absolute'):\n",
    "        ''' Use historical returns as dataset, draw sample from it to simulate the \n",
    "            potential loss (VaR)\n",
    "\n",
    "            Parameter:\n",
    "                draw_num: number of path simulated\n",
    "                p_name: portfolio name\n",
    "                VaROption: 'Absolute' VaR or 'Relative' VaR\n",
    "        '''\n",
    "        stocks = self.__portfolio.Stock.values # stocks of portfilio\n",
    "        rt=self.__returns[stocks] # return of each stock of the portfilio\n",
    "        portfolio=self.__portfolio.set_index('Stock') # set the portfolio index to be Stock\n",
    "        holding=portfolio.loc[stocks].Holding # get holding of each stock of the portfilio\n",
    "        current_price=self.__price[stocks].iloc[-1,:] # get current price of each stock of the portfilio\n",
    "\n",
    "        # sampling from the historical returns\n",
    "        size=draw_num\n",
    "        historical_rt=rt.sample(n=size,replace=True)\n",
    "\n",
    "        # simulated price\n",
    "        simulate_price = np.expand_dims(current_price,1).repeat(draw_num,axis=1) * (historical_rt.T)\n",
    "        # simulated position\n",
    "        simulate_position=np.expand_dims(holding,1).repeat(draw_num,axis=1) * simulate_price\n",
    "        # simulated portfolio value\n",
    "        simulate_PV=pd.DataFrame(simulate_position).sum()\n",
    "        # sort\n",
    "        simulate_PV=pd.DataFrame(simulate_PV.sort_values(ascending=True))\n",
    "        \n",
    "        if VaROption==\"Absolute\":\n",
    "            VaR_p=-np.quantile(simulate_PV,alpha)\n",
    "        elif VaROption=='Relative':\n",
    "            VaR_p=simulate_PV.mean()-np.quantile(simulate_PV,alpha)\n",
    "        else:\n",
    "            raise Exception(\"Unknown VaROption!\")\n",
    "\n",
    "        if plot:\n",
    "            # add a column of current Portfolio value\n",
    "            simulate_PV[1]=simulate_PV\n",
    "            simulate_PV[0]=0\n",
    "            plot_data=simulate_PV.T\n",
    "            # plot\n",
    "            fig, ax = plt.subplots(1,2,figsize=(14,6))\n",
    "            plot_data.plot(ax=ax[0],legend=False,xlabel='Time',ylabel='Price',title=\"Historical Simulation({} path) for portfolio {}\".format(draw_num,p_name))\n",
    "            sns.histplot(data=simulate_PV[1],kde=True,stat=\"density\",ax=ax[1])\n",
    "\n",
    "            # fill VaR area\n",
    "            # Get the two lines from the axes to generate shading\n",
    "            l = ax[1].lines[0]\n",
    "            # Get the xy data from the lines so that we can shade\n",
    "            x = l.get_xydata()[:,0]\n",
    "            y = l.get_xydata()[:,1]\n",
    "            ax[1].fill_between(x,y,where=x<-VaR_p,color=\"red\", alpha=0.3)\n",
    "\n",
    "            # plot the VaR\n",
    "            plt.axvline( x=-VaR_p ,color='#FF6347')\n",
    "            plt.title(\"Historical Simulated VaR($) of Portfolio {}\".format(p_name))\n",
    "            plt.legend(['Historical simulation kde','VaR'])\n",
    "\n",
    "        return VaR_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the DailyPrices data into dataframe\n",
    "price=pd.read_csv('DailyPrices.csv',index_col='Date')\n",
    "# calculate the returns\n",
    "rt=return_calculate(price)\n",
    "# information about portfolio\n",
    "portfolio=pd.read_csv('portfolio.csv',index_col='Portfolio')\n",
    "\n",
    "# Delta Normal + EWMA\n",
    "A=VaR_portfolio(portfolio.loc['A'],rt,price).delta_normal(0.05)\n",
    "print(\"VaR of portfolio A is {}\".format(A))\n",
    "B=VaR_portfolio(portfolio.loc['B'],rt,price).delta_normal(0.05)\n",
    "print(\"VaR of portfolio B is {}\".format(B))\n",
    "C=VaR_portfolio(portfolio.loc['C'],rt,price).delta_normal(0.05)\n",
    "print(\"VaR of portfolio C is {}\".format(C))\n",
    "All=VaR_portfolio(portfolio,rt,price).delta_normal(0.05)\n",
    "print(\"VaR of the whole portfolio is {}\".format(All))\n",
    "\n",
    "# Normal Monte Carlo Simulation \n",
    "A=VaR_portfolio(portfolio.loc['A'],rt,price).normal_MC(plot=True,p_name='A')\n",
    "print(\"VaR of portfolio A is {}\".format(A))\n",
    "B=VaR_portfolio(portfolio.loc['B'],rt,price).normal_MC(plot=True,p_name='B')\n",
    "print(\"VaR of portfolio B is {}\".format(B))\n",
    "C=VaR_portfolio(portfolio.loc['C'],rt,price).normal_MC(plot=True,p_name='C')\n",
    "print(\"VaR of portfolio C is {}\".format(C))\n",
    "All=VaR_portfolio(portfolio,rt,price).normal_MC(plot=True,p_name='All')\n",
    "print(\"VaR of the whole portfolio is {}\".format(All))\n",
    "\n",
    "# Historical Simulation \n",
    "A=VaR_portfolio(portfolio.loc['A'],rt,price).historical_simulation(plot=True,p_name='A')\n",
    "print(\"VaR of portfolio A is {}\".format(A))\n",
    "B=VaR_portfolio(portfolio.loc['B'],rt,price).historical_simulation(plot=True,p_name='B')\n",
    "print(\"VaR of portfolio B is {}\".format(B))\n",
    "C=VaR_portfolio(portfolio.loc['C'],rt,price).historical_simulation(plot=True,p_name='C')\n",
    "print(\"VaR of portfolio C is {}\".format(C))\n",
    "All=VaR_portfolio(portfolio,rt,price).historical_simulation(plot=True,p_name='All')\n",
    "print(\"VaR of the whole portfolio is {}\".format(All))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Generalized Fitted Model (Contain Normal, T or other distribution you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FittedModel:\n",
    "    '''The prototype of fitted distribution.'''\n",
    "    def __init__(self):\n",
    "        self.dist=self.set_dist() # the distribution\n",
    "        self.frz_dist=None # the distribution which has specific parameters\n",
    "\n",
    "    def set_dist(self):\n",
    "        '''Need to be implemented in subclass to set the dist.'''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def freeze_dist(self,parameters):\n",
    "        '''Need to be implemented in subclass to set the parameters of different distribution.'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def fit(self,data,x0,cons):\n",
    "        '''\n",
    "        the data is np.array\n",
    "        Use MLE to fit the distribution\n",
    "        x0 is initial paremeters which needed to be implemented in subclass\n",
    "        cons is constraints of parameters which needed to be implemented in subclass\n",
    "        '''\n",
    "        def nll(parameters,x):\n",
    "            '''Negative likelihood function'''\n",
    "            self.freeze_dist(parameters)\n",
    "            ll=self.frz_dist.logpdf(x=x).sum()\n",
    "            return -ll\n",
    "        MLE = minimize(nll, x0=x0, args=data, constraints = cons) # MLE \n",
    "        self.freeze_dist(MLE.x)\n",
    "        self.fitted_parameters=MLE.x\n",
    "\n",
    "    @property\n",
    "    def fitted_dist(self):\n",
    "        '''Return Fitted Distribution'''\n",
    "        return self.frz_dist\n",
    "\n",
    "class Norm(FittedModel):\n",
    "    def set_dist(self):\n",
    "        '''set the distribution to be normal'''\n",
    "        return stats.norm\n",
    "        \n",
    "    def freeze_dist(self,parameters):\n",
    "        '''set the parameters of norm: parameters[0]--mu, parameters[1]--std'''\n",
    "        self.frz_dist=self.dist(loc=parameters[0],scale=parameters[1])\n",
    "\n",
    "    def fit(self,data):\n",
    "        '''set the initial parameters and cons to call the father's fit'''\n",
    "        x0 = (data.mean(),data.std())  # initial paremeters\n",
    "        cons = [ {'type':'ineq', 'fun':lambda x:x[1]} ] # standard deviation is non-negative\n",
    "        super().fit(data,x0,cons)\n",
    "\n",
    "class T(FittedModel):\n",
    "    def set_dist(self):\n",
    "        '''set the distribution to be normal'''\n",
    "        return stats.t\n",
    "        \n",
    "    def freeze_dist(self,parameters):\n",
    "        '''set the parameters of norm: parameters[0]--degree of freedom, parameters[1]--mu, parameters[2]--std'''\n",
    "        self.frz_dist=self.dist(df=parameters[0],loc=parameters[1],scale=parameters[2])\n",
    "        \n",
    "    def fit(self,data):\n",
    "        '''set the initial parameters and cons to call the father's fit'''\n",
    "        # degree of freedom of t should be greater than 2; standard deviation is non-negative\n",
    "        cons=[ {'type':'ineq', 'fun':lambda x:x[0]-2} , {'type':'ineq', 'fun':lambda x:x[2]} ] \n",
    "        x0 = np.array([2,0,1]) # initial parameter\n",
    "        super().fit(data,x0,cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('problem1.csv') # read the data\n",
    "data=np.array(data.values).reshape(data.size) # transform to array\n",
    "\n",
    "# Fit the normal distribution\n",
    "norm=Norm()\n",
    "norm.fit(data)\n",
    "fitted_norm=norm.fitted_dist\n",
    "# Fit the T distribution\n",
    "t=T()\n",
    "t.fit(data)\n",
    "fitted_t=t.fitted_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Metrics (1. Var & ES using distribution 2. Var & ES using historical data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskMetrics:\n",
    "    @staticmethod\n",
    "    def VaR_dist(dist,alpha=0.05):\n",
    "        '''Given a distribution and alpha, calculate the corresponding VaR'''\n",
    "        return -dist.ppf(alpha)\n",
    "\n",
    "    @staticmethod\n",
    "    def ES_dist(dist,alpha=0.05):\n",
    "        '''Given a distribution and alpha, calculate the corresponding Expected Shortfall'''\n",
    "        lb=-np.inf   \n",
    "        ub=dist.ppf(alpha)\n",
    "        return -dist.expect(lb=lb,ub=ub)/alpha # integral\n",
    "    \n",
    "    @staticmethod\n",
    "    def VaR_historical(data,alpha=0.05):\n",
    "        '''Given a dataset(array), calculate the its historical VaR'''\n",
    "        data.sort()\n",
    "        n=round(data.size*alpha)\n",
    "        return -data[n-1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def ES_historical(data,alpha=0.05):\n",
    "        '''Given a dataset(array), calculate the its historical Expected Shortfall'''\n",
    "        data.sort()\n",
    "        n=round(data.size*alpha)\n",
    "        return -data[:n].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the VaR & ES using distribution\n",
    "VaR_normal=RiskMetrics.VaR_dist(fitted_norm)\n",
    "ES_normal=RiskMetrics.ES_dist(fitted_norm)\n",
    "VaR_t=RiskMetrics.VaR_dist(fitted_t)\n",
    "ES_t=RiskMetrics.ES_dist(fitted_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitter (Fit the dataframe with Model, return a 1-D dataframe of fitted distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFitter:\n",
    "    ''' Fit the data with Model, return a 1D Dataframe of fitted distributions\n",
    "\n",
    "        Parameters:\n",
    "            FittedModel(Class) ---- a subclass of FittedModel class\n",
    "\n",
    "        Usage:\n",
    "            dists=ModelFitter(FittedModel).fit(data)\n",
    "    '''\n",
    "\n",
    "    def __init__(self,FittedModel):\n",
    "        ''' Initialize the model within the class to fit all the data.'''\n",
    "        self.model=FittedModel()\n",
    "    \n",
    "    def fit(self,data):\n",
    "        '''Fit all the data with the model inside the Fitter\n",
    "            Data(Dataframe) -- return of stock\n",
    "        '''\n",
    "        dists=[]\n",
    "        for name in data.columns:\n",
    "            rt=np.array(data[name].values)\n",
    "            self.model.fit(rt)\n",
    "            dists.append(self.model.fitted_dist)\n",
    "        dists=pd.DataFrame(dists).T\n",
    "        dists.columns=data.columns\n",
    "        dists.index=[\"distribution\"]\n",
    "        return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = portfolio.Stock.values # stocks of portfilio\n",
    "rt=returns[stocks] # return of each stock of the portfilio\n",
    "portfolio=portfolio.set_index('Stock') # set the portfolio index to be Stock\n",
    "holding=portfolio.loc[stocks].Holding # get holding of each stock of the portfilio\n",
    "current_price=price[stocks].iloc[-1,:] # get current price of each stock of the portfilio\n",
    "\n",
    "# Fit the data with Model, return a 1D Dataframe of fitted distributions\n",
    "dists=ModelFitter(dist).fit(rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Copula (Simulate whatever distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianCopula:\n",
    "    ''' Construct the Gaussian Copula to simulate\n",
    "    \n",
    "        Parameters:\n",
    "            dists(DataFrame) --- a group of distributions (1D Dataframe of fitted distributions)\n",
    "            data(DataFrame) --- the data that fits the distributions will be used to generate the simulated sample\n",
    "\n",
    "        Usage:\n",
    "            copula=GaussianCopula(dists,data)\n",
    "            sample=copula.simulate()\n",
    "    '''\n",
    "    def __init__(self,dists,data):\n",
    "        self.models=dists\n",
    "        self.data=data\n",
    "    \n",
    "    def simulate(self,NSim=5000):\n",
    "        transform_data=pd.DataFrame()\n",
    "        for name in self.data.columns:\n",
    "            rt=np.array(self.data[name])\n",
    "            # Use the CDF to transform the data to uniform universe\n",
    "            # Use the standard normal quantile function to transform the uniform to normal \n",
    "            transform_data[name]=stats.norm.ppf(self.models[name][0].cdf(rt))\n",
    "        # Spearman correlation\n",
    "        corr_spearman = stats.spearmanr(transform_data,axis=0)[0]\n",
    "        # Use PCA simulation\n",
    "        simulator = Simulator(corr_spearman,NSim)\n",
    "        # Simulate Normal & Transform to uniform\n",
    "        SimU=stats.norm.cdf(simulator.PCA_Simulation(1),loc=0,scale=1)\n",
    "        # Transform to Model Distribution\n",
    "        simulatedResults = pd.DataFrame()\n",
    "        for idx,name in enumerate(self.data.columns):\n",
    "            simulatedResults[name] = self.models[name][0].ppf(SimU[idx,:])\n",
    "        return simulatedResults.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data with Model to get a group of distributions\n",
    "dists=ModelFitter(dist).fit(rt)\n",
    "# Construct Copula\n",
    "copula=GaussianCopula(dists,rt)\n",
    "# Simulate\n",
    "simulated_rt=copula.simulate(NSim=draw_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VaR for Portfolio (升级版)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VaR_portfolio:\n",
    "    '''\n",
    "    Differnet Method to get the VaR of stock portfolio\n",
    "    \n",
    "        1. delta_normal\n",
    "        2. normal_MC\n",
    "        3. historical_simulation\n",
    "\n",
    "    The fromat of portfolio, returns, price should be same as the file in the current directory.\n",
    "\n",
    "    Usage:\n",
    "        All=VaR_portfolio(portfolio,rt,price).delta_normal(0.05)\n",
    "        All=VaR_portfolio(portfolio,rt,price).normal_MC(plot=True,p_name='All')\n",
    "        All=VaR_portfolio(portfolio,rt,price).historical_simulation(plot=True,p_name='All')\n",
    "    '''\n",
    "    # initialization\n",
    "    def __init__(self,portfolio,returns,price):\n",
    "        '''The fromat of data should be same as the file in the current directory.'''\n",
    "        # information about portfolio\n",
    "        self.__portfolio=portfolio\n",
    "        # returns information about companies\n",
    "        self.__returns=returns\n",
    "        # price about stock\n",
    "        self.__price=price\n",
    "\n",
    "    def delta_normal(self,alpha=0.05,option='EWMA'):\n",
    "        ''' Delta Normal method to calculate the VaR of the Stock portfolio\n",
    "            \n",
    "            Parmeter:\n",
    "                option='EWMA' or 'std'\n",
    "        '''\n",
    "        stocks = self.__portfolio.Stock.values # stocks of portfilio\n",
    "        rt=self.__returns[stocks] # return of each stock of the portfilio\n",
    "        portfolio=self.__portfolio.set_index('Stock') # set the portfolio index to be Stock\n",
    "        holding=portfolio.loc[stocks].Holding # get holding of each stock of the portfilio\n",
    "        current_price=self.__price[stocks].iloc[-1,:] # get current price of each stock of the portfilio\n",
    "        current_position = holding * current_price # current position of each stock of the portfilio\n",
    "        PV = current_position.sum() # portfolio value\n",
    "        delta=current_position/PV # calculate the delta\n",
    "        stocks = self.__portfolio.Stock.values\n",
    "        rt=self.__returns[stocks]\n",
    "\n",
    "        if option=='EWMA':\n",
    "            # Calculate EWMA covariance\n",
    "            model=EWMA(rt.values,0.94) # assume lambda = 0.94\n",
    "            cov_mat=model.cov_mat\n",
    "        elif option=='std':\n",
    "            # Calculate standard deviation\n",
    "            cov_mat=rt.cov()\n",
    "\n",
    "        # calculate the std of portfolio\n",
    "        std = np.sqrt(delta @ cov_mat @ delta)\n",
    "        # get the VaR of the portfolio\n",
    "        VaR_p = PV*(-stats.norm.ppf(alpha))*std\n",
    "        return VaR_p\n",
    "\n",
    "    def normal_MC(self,alpha=0.05,option='EWMA',method='PCA',draw_num=100000,pct=1,plot=False,VaROption='Absolute',p_name=''):\n",
    "        ''' Use Monte Carlo Methods to simulate the price of each stock of portfolio\n",
    "            then calculate the value of the portfolio\n",
    "            \n",
    "            Parmeter:\n",
    "                option: 'EWMA' or 'std'\n",
    "                method: 'PCA' or 'Cholesky'\n",
    "                VaROption: 'Absolute' VaR or 'Relative' VaR\n",
    "                draw_num: number of path simulated\n",
    "                pct: the percentage of variance expained by PCA\n",
    "                plot: plot the simulated path or not\n",
    "                alpha: 1-alpha is confidence level\n",
    "                p_name: portfolio name\n",
    "        '''\n",
    "        stocks = self.__portfolio.Stock.values # stocks of portfilio\n",
    "        rt=self.__returns[stocks] # return of each stock of the portfilio\n",
    "        portfolio=self.__portfolio.set_index('Stock') # set the portfolio index to be Stock\n",
    "        holding=portfolio.loc[stocks].Holding # get holding of each stock of the portfilio\n",
    "        current_price=self.__price[stocks].iloc[-1,:] # get current price of each stock of the portfilio\n",
    "\n",
    "        # get the covariance\n",
    "        if option=='EWMA':\n",
    "            # Calculate EWMA covariance\n",
    "            model=EWMA(rt.values,0.94) # assume lambda = 0.94\n",
    "            cov_mat=model.cov_mat\n",
    "        elif option=='std':\n",
    "            # Calculate standard deviation\n",
    "            cov_mat=rt.cov()\n",
    "        else:\n",
    "            raise Exception(\"Unknown option!\")\n",
    "        \n",
    "        # MC to get the simulated return\n",
    "        simulator = Simulator(cov_mat,draw_num)\n",
    "        if method=='PCA':\n",
    "            simulated_rt=simulator.PCA_Simulation(pct)\n",
    "        elif method=='Cholesky':\n",
    "            simulated_rt=simulator.DirectSimulation()\n",
    "        else:\n",
    "            raise Exception(\"Unknown method!\")\n",
    "        # simulated price\n",
    "        simulate_price = np.expand_dims(current_price,1).repeat(draw_num,axis=1) * simulated_rt\n",
    "        # simulated position\n",
    "        simulate_position=np.expand_dims(holding,1).repeat(draw_num,axis=1) * simulate_price\n",
    "        # simulated portfolio value\n",
    "        simulate_PV=pd.DataFrame(simulate_position).sum()\n",
    "        # sort\n",
    "        simulate_PV=pd.DataFrame(simulate_PV.sort_values(ascending=True))\n",
    "        self.simulate_PV=simulate_PV\n",
    "\n",
    "        if VaROption==\"Absolute\":\n",
    "            VaR_p=-np.quantile(simulate_PV,alpha)\n",
    "        elif VaROption=='Relative':\n",
    "            VaR_p=simulate_PV.mean()-np.quantile(simulate_PV,alpha)\n",
    "        else:\n",
    "            raise Exception(\"Unknown VaROption!\")\n",
    "        \n",
    "        \n",
    "        if plot:\n",
    "            # add current Portfolio value\n",
    "            simulate_PV[1]=simulate_PV\n",
    "            simulate_PV[0]=0\n",
    "            plot_data=simulate_PV.T\n",
    "            # plot\n",
    "            fig, ax = plt.subplots(1,2,figsize=(14,6))\n",
    "            plot_data.plot(ax=ax[0],legend=False,xlabel='Time',ylabel='Price',title=\"Mote Carlo Simulation({} path) for portfolio {}\".format(draw_num,p_name))\n",
    "            sns.histplot(data=simulate_PV[1],kde=True,stat=\"density\",ax=ax[1])\n",
    "\n",
    "            # fill VaR area\n",
    "            # Get the two lines from the axes to generate shading\n",
    "            l = ax[1].lines[0]\n",
    "            # Get the xy data from the lines so that we can shade\n",
    "            x = l.get_xydata()[:,0]\n",
    "            y = l.get_xydata()[:,1]\n",
    "            ax[1].fill_between(x,y,where=x<-VaR_p,color=\"red\", alpha=0.3)\n",
    "            \n",
    "            # plot the VaR\n",
    "            plt.axvline(-VaR_p,color='#FF6347')\n",
    "            plt.title(\"Monte Carlo Simulated VaR($) of Portfolio {}\".format(p_name))\n",
    "            plt.legend(['MC simulation kde','VaR'])\n",
    "        return VaR_p\n",
    "\n",
    "    def historical_simulation(self,alpha=0.05,draw_num=10000,plot=False,p_name='',VaROption='Absolute'):\n",
    "        ''' Use historical returns as dataset, draw sample from it to simulate the \n",
    "            potential loss (VaR)\n",
    "\n",
    "            Parameter:\n",
    "                draw_num: number of path simulated\n",
    "                p_name: portfolio name\n",
    "                VaROption: 'Absolute' VaR or 'Relative' VaR\n",
    "        '''\n",
    "        stocks = self.__portfolio.Stock.values # stocks of portfilio\n",
    "        rt=self.__returns[stocks] # return of each stock of the portfilio\n",
    "        portfolio=self.__portfolio.set_index('Stock') # set the portfolio index to be Stock\n",
    "        holding=portfolio.loc[stocks].Holding # get holding of each stock of the portfilio\n",
    "        current_price=self.__price[stocks].iloc[-1,:] # get current price of each stock of the portfilio\n",
    "\n",
    "        # sampling from the historical returns\n",
    "        size=draw_num\n",
    "        historical_rt=rt.sample(n=size,replace=True)\n",
    "\n",
    "        # simulated price\n",
    "        simulate_price = np.expand_dims(current_price,1).repeat(draw_num,axis=1) * (historical_rt.T)\n",
    "        # simulated position\n",
    "        simulate_position=np.expand_dims(holding,1).repeat(draw_num,axis=1) * simulate_price\n",
    "        # simulated portfolio value\n",
    "        simulate_PV=pd.DataFrame(simulate_position).sum()\n",
    "        # sort\n",
    "        simulate_PV=pd.DataFrame(simulate_PV.sort_values(ascending=True))\n",
    "        self.simulate_PV=simulate_PV\n",
    "\n",
    "        if VaROption==\"Absolute\":\n",
    "            VaR_p=-np.quantile(simulate_PV,alpha)\n",
    "        elif VaROption=='Relative':\n",
    "            VaR_p=simulate_PV.mean()-np.quantile(simulate_PV,alpha)\n",
    "        else:\n",
    "            raise Exception(\"Unknown VaROption!\")\n",
    "\n",
    "        if plot:\n",
    "            # add a column of current Portfolio value\n",
    "            simulate_PV[1]=simulate_PV\n",
    "            simulate_PV[0]=0\n",
    "            plot_data=simulate_PV.T\n",
    "            # plot\n",
    "            fig, ax = plt.subplots(1,2,figsize=(14,6))\n",
    "            plot_data.plot(ax=ax[0],legend=False,xlabel='Time',ylabel='Price',title=\"Historical Simulation({} path) for portfolio {}\".format(draw_num,p_name))\n",
    "            sns.histplot(data=simulate_PV[1],kde=True,stat=\"density\",ax=ax[1])\n",
    "\n",
    "            # fill VaR area\n",
    "            # Get the two lines from the axes to generate shading\n",
    "            l = ax[1].lines[0]\n",
    "            # Get the xy data from the lines so that we can shade\n",
    "            x = l.get_xydata()[:,0]\n",
    "            y = l.get_xydata()[:,1]\n",
    "            ax[1].fill_between(x,y,where=x<-VaR_p,color=\"red\", alpha=0.3)\n",
    "\n",
    "            # plot the VaR\n",
    "            plt.axvline( x=-VaR_p ,color='#FF6347')\n",
    "            plt.title(\"Historical Simulated VaR($) of Portfolio {}\".format(p_name))\n",
    "            plt.legend(['Historical simulation kde','VaR'])\n",
    "\n",
    "        return VaR_p\n",
    "\n",
    "    def Copula_MC(self,dist,alpha=0.05,draw_num=10000,pct=1,plot=False,VaROption='Absolute',p_name=''):\n",
    "        ''' Use Copula Monte Carlo Methods(PCA) to simulate the price of each stock of portfolio\n",
    "            then calculate the value of the portfolio\n",
    "            \n",
    "            Parmeter:\n",
    "                VaROption: 'Absolute' VaR or 'Relative' VaR\n",
    "                draw_num: number of path simulated\n",
    "                plot: plot the simulated path or not\n",
    "                alpha: 1-alpha is confidence level\n",
    "                p_name: portfolio name\n",
    "        '''\n",
    "        stocks = self.__portfolio.Stock.values # stocks of portfilio\n",
    "        rt=self.__returns[stocks] # return of each stock of the portfilio\n",
    "        portfolio=self.__portfolio.set_index('Stock') # set the portfolio index to be Stock\n",
    "        holding=portfolio.loc[stocks].Holding # get holding of each stock of the portfilio\n",
    "        current_price=self.__price[stocks].iloc[-1,:] # get current price of each stock of the portfilio\n",
    "\n",
    "        # Fit the data with Model to get a group of distributions\n",
    "        dists=ModelFitter(dist).fit(rt)\n",
    "        # Construct Copula\n",
    "        copula=GaussianCopula(dists,rt)\n",
    "        # Simulate\n",
    "        simulated_rt=copula.simulate(NSim=draw_num)\n",
    "        # simulated price\n",
    "        simulate_price = np.expand_dims(current_price,1).repeat(draw_num,axis=1) * simulated_rt\n",
    "        # simulated position\n",
    "        simulate_position=np.expand_dims(holding,1).repeat(draw_num,axis=1) * simulate_price\n",
    "        # simulated portfolio value\n",
    "        simulate_PV=pd.DataFrame(simulate_position).sum()\n",
    "        # sort\n",
    "        simulate_PV=pd.DataFrame(simulate_PV.sort_values(ascending=True))\n",
    "        self.simulate_PV=simulate_PV\n",
    "\n",
    "        if VaROption==\"Absolute\":\n",
    "            VaR_p=-np.quantile(simulate_PV,alpha)\n",
    "        elif VaROption=='Relative':\n",
    "            VaR_p=simulate_PV.mean()-np.quantile(simulate_PV,alpha)\n",
    "        else:\n",
    "            raise Exception(\"Unknown VaROption!\")\n",
    "        \n",
    "        \n",
    "        if plot:\n",
    "            # add current Portfolio value\n",
    "            simulate_PV[1]=simulate_PV\n",
    "            simulate_PV[0]=0\n",
    "            plot_data=simulate_PV.T\n",
    "            # plot\n",
    "            fig, ax = plt.subplots(1,2,figsize=(14,6))\n",
    "            plot_data.plot(ax=ax[0],legend=False,xlabel='Time',ylabel='Price',title=\"Copula Mote Carlo Simulation({} path) for portfolio {}\".format(draw_num,p_name))\n",
    "            sns.histplot(data=simulate_PV[1],kde=True,stat=\"density\",ax=ax[1])\n",
    "\n",
    "            # fill VaR area\n",
    "            # Get the two lines from the axes to generate shading\n",
    "            l = ax[1].lines[0]\n",
    "            # Get the xy data from the lines so that we can shade\n",
    "            x = l.get_xydata()[:,0]\n",
    "            y = l.get_xydata()[:,1]\n",
    "            ax[1].fill_between(x,y,where=x<-VaR_p,color=\"red\", alpha=0.3)\n",
    "            \n",
    "            # plot the VaR\n",
    "            plt.axvline(-VaR_p,color='#FF6347',linestyle='--')\n",
    "            plt.title(\"Copula Monte Carlo Simulated VaR($) of Portfolio {}\".format(p_name))\n",
    "            plt.legend(['Copula MC simulation kde','VaR'])\n",
    "        \n",
    "        return VaR_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllPortfolio=VaR_portfolio(portfolio,rt,price)\n",
    "VaR_All=AllPortfolio.Copula_MC(T_mean0,plot=True,p_name='All')\n",
    "ES_All=RiskMetrics.ES_historical(np.array(AllPortfolio.simulate_PV[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option valuation via integral -- Generalized Black Scholes Merton for European Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integral_gbsm(s,strike,t,vol,rf,c,tradingDayYear,call=True):\n",
    "    ''' Option valuation via integral -- Generalized Black Scholes Merton\n",
    "        European Style.  Assumed LogNormal Prices\n",
    "        rf = c       -- Black Scholes 1973\n",
    "        c = rf - q   -- Merton 1973 stock model where q is the continous dividend yield\n",
    "        c = 0        -- Black 1976 futures option model\n",
    "        c,r = 0      -- Asay 1982 margined futures option model\n",
    "        c = rf - rff -- Garman and Kohlhagen 1983 currency option model where rff is the risk free rate of the foreign currency\n",
    "\n",
    "        s - Underlying Price\n",
    "        strike - Strike Price\n",
    "        t - days until maturity\n",
    "        rf - Risk free rate\n",
    "        vol - Yearly Volatility\n",
    "        c - Cost of Carry\n",
    "        tradingDayYear - trading days in a year\n",
    "        call - Call valuation if set True\n",
    "    '''\n",
    "\n",
    "    ttm = t/tradingDayYear # time to maturity\n",
    "    dailyVol = vol/np.sqrt(tradingDayYear) # daily volatility\n",
    "    sigma = dailyVol*np.sqrt(t)\n",
    "    mu = np.log(s)+ttm*c-sigma**2/2 \n",
    "\n",
    "    dist = lognorm(sigma,0,np.exp(mu))\n",
    "    f = lambda x: 2*dist.pdf(x)\n",
    "\n",
    "    if call:\n",
    "        f = lambda x: max(x-strike,0) * dist.pdf(x)\n",
    "        value = np.exp(-ttm*rf)*integrate.quad(f,0,strike*100)[0]\n",
    "    else:\n",
    "        f = lambda x: max(strike-x,0)*dist.pdf(x)\n",
    "        value = np.exp(-ttm*rf)*integrate.quad(f,0,strike*100)[0]\n",
    "    return value\n",
    "\n",
    "    '''\n",
    "    # lognormal of numpy is a liitle bit strange. \n",
    "\n",
    "        # In mathematical notation if X is N(mu,sigma) then Y=exp(X) is LogN(mu, sigma). To get X in scipy I would use norm(mu,sigma) but to get Y I would use lognorm(sigma, 0, exp(mu))\n",
    "\n",
    "        # X = exp(mu + sigma * Z), Z is standard normal\n",
    "        # which is the same as: X = exp(mu) * exp(Z)**sigma      \n",
    "        # This can be sneakily re-written as follows: X = exp(mu) * exp(Z-Z0)**sigma where Z0 = 0\n",
    "        # This equation is of the form: f(x) = a * ( (x-x0) ** b ) = scale * ( (x-location) ** shape )\n",
    "    '''\n",
    "    '''\n",
    "    Problem: \n",
    "        we could not directly use np.inf as upper bound because the quad would generate wrong result.\n",
    "        \n",
    "        Solutions and explanations:\n",
    "        For quad, be aware that pulse shapes and other sharp features as compared to the size of the integration interval may not be integrated correctly using this method. \n",
    "        For example, the function is using only two (last=2) intervals. You can add points to the interval to force the routine to use points closer to the left limit. Note that points is a sequence of break points in the bounded integration interval where local difficulties of the integrand may occur (e.g., singularities, discontinuities). In this case, I'm not using it to point out discontinuities, but rather to force quad to perform more integration steps near the left boundary.\n",
    "        quad use heuristic algorithms, using adaptative step of integration to reduce time computing. Where the function is flat, it goes faster. so on big global interval, it can miss the peak.\n",
    "        This happens because the adaptive quadrature routine implemented in quad, while working as designed, does not notice the small, important part of the function within such a large, finite interval. For best results, consider using integration limits that tightly surround the important part of the integrand.\n",
    "        Integrands with several important regions can be broken into pieces as necessary.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalize Black Scholes Merton Closed Formula for European Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbsm(s,strike,t,vol,rf,c,tradingDayYear,call=True):\n",
    "    ''' Generalize Black Scholes Merton\n",
    "        rf = c       -- Black Scholes 1973\n",
    "        c = rf - q   -- Merton 1973 stock model where q is the continous dividend yield\n",
    "        c = 0        -- Black 1976 futures option model\n",
    "        c,r = 0      -- Asay 1982 margined futures option model\n",
    "        c = rf - rff -- Garman and Kohlhagen 1983 currency option model where rff is the risk free rate of the foreign currency\n",
    "\n",
    "        Option valuation via BSM closed formula\n",
    "        European Style.  Assumed LogNormal Prices\n",
    "        s - Underlying Price\n",
    "        strike - Strike Price\n",
    "        t - days until maturity\n",
    "        rf - Risk free rate\n",
    "        vol - Yearly Volatility\n",
    "        c - Cost of Carry\n",
    "        tradingDayYear - trading days in a year\n",
    "        call - Call valuation if set True\n",
    "    '''\n",
    "    ttm=t/tradingDayYear\n",
    "    d1=(np.log(s/strike)+(c+vol**2/2)*ttm)/vol/np.sqrt(ttm)\n",
    "    d2=d1-vol*np.sqrt(ttm)\n",
    "    if call:\n",
    "        return s*np.exp((c-rf)*ttm)*norm.cdf(d1)-strike*np.exp(-rf*ttm)*norm.cdf(d2)\n",
    "    else:\n",
    "        return strike*np.exp(-rf*ttm)*norm.cdf(-d2)-s*np.exp((c-rf)*ttm)*norm.cdf(-d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## European Option valuation via Binomial Trees (No dividends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial_tree_gbsm(s,strike,t,vol,rf,c,tradingDayYear,N,call=True):\n",
    "    '''\n",
    "    Generalize Black Scholes Merton\n",
    "    rf = c       -- Black Scholes 1973\n",
    "    c = rf - q   -- Merton 1973 stock model where q is the continous dividend yield\n",
    "    c = 0        -- Black 1976 futures option model\n",
    "    c,r = 0      -- Asay 1982 margined futures option model\n",
    "    c = rf - rff -- Garman and Kohlhagen 1983 currency option model where rff is the risk free rate of the foreign currency\n",
    "\n",
    "    Option valuation via Binomial Trees\n",
    "    European Style.  Assumed LogNormal Prices\n",
    "    s - Underlying Price\n",
    "    strike - Strike Price\n",
    "    t - days until maturity\n",
    "    rf - Risk free rate\n",
    "    vol - Yearly Volatility\n",
    "    c - Cost of Carry\n",
    "    tradingDayYear - trading days in a year\n",
    "    N - Steps of Binomial Tree\n",
    "    call - Call valuation if set True\n",
    "    '''\n",
    "    ttm=t/tradingDayYear\n",
    "    delta_t=ttm/N\n",
    "    # price multiplier in the (positive, negtive) case\n",
    "    u=np.exp(vol*np.sqrt(delta_t)) \n",
    "    d=np.exp(-vol*np.sqrt(delta_t))\n",
    "    # probability of up/down move in price\n",
    "    pu=(np.exp(c*delta_t)-d)/(u-d)\n",
    "    pd=1-pu\n",
    "    S=[] # stock price\n",
    "    P=[] # probability of each case\n",
    "    nPaths=[] # number of paths of each case\n",
    "    for i in range(N+1):\n",
    "        S.append(s*(u**i)*(d**(N-i)))\n",
    "        P.append((pu**i)*(pd**(N-i)))\n",
    "        nPaths.append(math.factorial(N)/math.factorial(i)/math.factorial(N-i))\n",
    "    # turn list into array\n",
    "    S=np.array(S)\n",
    "    P=np.array(P)\n",
    "    nPaths=np.array(nPaths)\n",
    "    # function to convert stock price to value of option\n",
    "    f=lambda x,y: x-y if x-y>0 else 0\n",
    "    vfunc = np.vectorize(f,otypes=[float])\n",
    "    if call:\n",
    "        Pi=vfunc(S,strike)\n",
    "    else:\n",
    "        Pi=vfunc(strike,S)\n",
    "    return np.exp(-rf*ttm)*(Pi*P)@nPaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## European Option valuation via Monte Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_gbsm(s,strike,t,vol,rf,c,tradingDayYear,N,call=True):\n",
    "    ''' Generalize Black Scholes Merton\n",
    "        rf = c       -- Black Scholes 1973\n",
    "        c = rf - q   -- Merton 1973 stock model where q is the continous dividend yield\n",
    "        c = 0        -- Black 1976 futures option model\n",
    "        c,r = 0      -- Asay 1982 margined futures option model\n",
    "        c = rf - rff -- Garman and Kohlhagen 1983 currency option model where rff is the risk free rate of the foreign currency\n",
    "\n",
    "        Option valuation via Monte Simulation\n",
    "        European Style.  Assumed LogNormal Prices\n",
    "        s - Underlying Price\n",
    "        strike - Strike Price\n",
    "        t - days until maturity\n",
    "        rf - Risk free rate\n",
    "        vol - Yearly Volatility\n",
    "        c - Cost of Carry\n",
    "        tradingDayYear - trading days in a year\n",
    "        N - Steps of Binomial Tree\n",
    "        call - Call valuation if set True\n",
    "    '''\n",
    "    ttm = t/tradingDayYear # time to maturity\n",
    "    dailyVol = vol/np.sqrt(tradingDayYear) # daily volatility\n",
    "    dist=norm(c/tradingDayYear-dailyVol**2/2,dailyVol)\n",
    "    Price=[]\n",
    "    for i in range(N):\n",
    "      rts=dist.rvs(t)\n",
    "      total_rt=np.exp(sum(rts))\n",
    "      Price.append(s*total_rt)\n",
    "    f=lambda x,y: x-y if x-y>0 else 0\n",
    "    vfunc = np.vectorize(f,otypes=[float])\n",
    "    if call:\n",
    "      Pi=vfunc(Price,strike)\n",
    "    else:\n",
    "      Pi=vfunc(strike,Price)\n",
    "    return np.exp(-rf*ttm)*Pi.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=165\n",
    "CurrentDate=datetime.datetime.strptime('03/03/2023','%m/%d/%Y')\n",
    "ExpirationDate=datetime.datetime.strptime('03/17/2023','%m/%d/%Y')\n",
    "dt=ExpirationDate-CurrentDate\n",
    "t=dt.days\n",
    "tradingDayYear=365\n",
    "rf=0.0425\n",
    "q=0.0053\n",
    "c=rf-q\n",
    "ttm=t/tradingDayYear\n",
    "N=100 # number of points in range of volatility\n",
    "strike_call=strike_put=160\n",
    "\n",
    "gbsm(s,strike_call,t,vol,rf,c,tradingDayYear,call=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implied Volatility for Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findImpliedVol(data):\n",
    "    ''' Find implied volatility\n",
    "        European Style.  Assumed LogNormal Prices\n",
    "        s - Underlying Price\n",
    "        strike - Strike Price\n",
    "        t - days until maturity\n",
    "        rf - Risk free rate\n",
    "        vol - Yearly Volatility\n",
    "        c - Cost of Carry\n",
    "        tradingDayYear - trading days in a year\n",
    "        N - Steps of Binomial Tree\n",
    "        call - Call valuation if set True\n",
    "\n",
    "        Generalize Black Scholes Merton\n",
    "        rf = c       -- Black Scholes 1973\n",
    "        c = rf - q   -- Merton 1973 stock model where q is the continous dividend yield\n",
    "        c = 0        -- Black 1976 futures option model\n",
    "        c,r = 0      -- Asay 1982 margined futures option model\n",
    "        c = rf - rff -- Garman and Kohlhagen 1983 currency option model where rff is the risk free rate of the foreign currency\n",
    "    '''\n",
    "    s=151.03\n",
    "    strike=data['Strike']\n",
    "    CurrentDate=datetime.datetime.strptime('03/03/2023','%m/%d/%Y')\n",
    "    ExpirationDate=data['Expiration']\n",
    "    ExpirationDate=datetime.datetime.strptime(ExpirationDate,'%m/%d/%Y')\n",
    "    dt=ExpirationDate-CurrentDate\n",
    "    t=dt.days\n",
    "    tradingDayYear=365\n",
    "    P=data['Last Price']\n",
    "    call=True if data['Type']=='Call' else False\n",
    "    rf=0.0425\n",
    "    q=0.0053\n",
    "    c=rf-q\n",
    "    ttm=t/tradingDayYear\n",
    "    def f(vol):\n",
    "        return gbsm(s,strike,t,vol,rf,c,tradingDayYear,call)-P  \n",
    "    iVol = optimize.root_scalar(f, bracket=[1e-6, 10], method='brentq')\n",
    "    return iVol.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAAPL_Options.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m currentPrice\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m151.03\u001b[39m\n\u001b[1;32m      3\u001b[0m iVol\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mapply(findImpliedVol,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('AAPL_Options.csv')\n",
    "currentPrice=151.03\n",
    "iVol=df.apply(findImpliedVol,axis=1)\n",
    "df['Implied Volatility']=iVol\n",
    "\n",
    "\n",
    "df.sort_values('Strike',inplace=True)\n",
    "ax=df.query(\"Type == 'Call'\").plot(x='Strike',y='Implied Volatility',kind='line',xlabel='Strike',ylabel='Implied Volatility',label='Call')\n",
    "df.query(\"Type == 'Put'\").plot(x='Strike',y='Implied Volatility',kind='line',xlabel='Strike',ylabel='Implied Volatility',ax=ax,label='Put')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implied Volatility ( If the type is not option, return np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findImpliedVol_plus(data):\n",
    "    '''Find implied volatility\n",
    "        European Style.  Assumed LogNormal Prices\n",
    "        s - Underlying Price\n",
    "        strike - Strike Price\n",
    "        t - days until maturity\n",
    "        rf - Risk free rate\n",
    "        vol - Yearly Volatility\n",
    "        c - Cost of Carry\n",
    "        tradingDayYear - trading days in a year\n",
    "        call - Call valuation if set True\n",
    "\n",
    "        Generalize Black Scholes Merton\n",
    "        rf = c       -- Black Scholes 1973\n",
    "        c = rf - q   -- Merton 1973 stock model where q is the continous dividend yield\n",
    "        c = 0        -- Black 1976 futures option model\n",
    "        c,r = 0      -- Asay 1982 margined futures option model\n",
    "        c = rf - rff -- Garman and Kohlhagen 1983 currency option model where rff is the risk free rate of the foreign currency\n",
    "    '''\n",
    "    if data['Type']!='Option':\n",
    "        return np.nan\n",
    "    s=151.03\n",
    "    strike=data['Strike']\n",
    "    CurrentDate=datetime.datetime.strptime('03/03/2023','%m/%d/%Y')\n",
    "    ExpirationDate=data['ExpirationDate']\n",
    "    ExpirationDate=datetime.datetime.strptime(ExpirationDate,'%m/%d/%Y')\n",
    "    dt=ExpirationDate-CurrentDate\n",
    "    t=dt.days\n",
    "    tradingDayYear=365\n",
    "    P=data['CurrentPrice']\n",
    "    call=True if data['OptionType']=='Call' else False\n",
    "    rf=0.0425\n",
    "    q=0.0053\n",
    "    c=rf-q\n",
    "    ttm=t/tradingDayYear\n",
    "    def f(vol):\n",
    "        return gbsm(s,strike,t,vol,rf,c,tradingDayYear,call)-P  \n",
    "    iVol = optimize.root_scalar(f, bracket=[1e-6, 10], method='brentq')\n",
    "    return iVol.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('problem3.csv')\n",
    "iVol=df.apply(findImpliedVol_plus,axis=1)\n",
    "df['Implied Volatility']=iVol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalize Black Scholes Merton Closed Formula for Portfolio Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbsm_plus(data,prices,days=0):\n",
    "    ''' Generalize Black Scholes Merton\n",
    "        rf = c       -- Black Scholes 1973\n",
    "        c = rf - q   -- Merton 1973 stock model where q is the continous dividend yield\n",
    "        c = 0        -- Black 1976 futures option model\n",
    "        c,r = 0      -- Asay 1982 margined futures option model\n",
    "        c = rf - rff -- Garman and Kohlhagen 1983 currency option model where rff is the risk free rate of the foreign currency\n",
    "\n",
    "        Option valuation via BSM closed formula\n",
    "        European Style.  Assumed LogNormal Prices\n",
    "        s - Underlying Price\n",
    "        strike - Strike Price\n",
    "        t - days until maturity\n",
    "        rf - Risk free rate\n",
    "        vol - Yearly Volatility\n",
    "        c - Cost of Carry\n",
    "        tradingDayYear - trading days in a year\n",
    "        call - Call valuation if set True\n",
    "    '''\n",
    "    if data['Type']!='Option':\n",
    "        return prices*data['Holding']\n",
    "\n",
    "    strike=data['Strike']\n",
    "    CurrentDate=datetime.datetime.strptime('03/03/2023','%m/%d/%Y')+datetime.timedelta(days=days)\n",
    "    ExpirationDate=data['ExpirationDate']\n",
    "    ExpirationDate=datetime.datetime.strptime(ExpirationDate,'%m/%d/%Y')\n",
    "    dt=ExpirationDate-CurrentDate\n",
    "    t=dt.days\n",
    "    tradingDayYear=365\n",
    "    P=data['CurrentPrice']\n",
    "    call=True if data['OptionType']=='Call' else False\n",
    "    rf=0.0425\n",
    "    q=0.0053\n",
    "    c=rf-q\n",
    "    ttm=t/tradingDayYear\n",
    "    vol = data['Implied Volatility']\n",
    "\n",
    "    positionValue=[]\n",
    "    for s in prices:\n",
    "        d1=(np.log(s/strike)+(c+vol**2/2)*ttm)/vol/np.sqrt(ttm)\n",
    "        d2=d1-vol*np.sqrt(ttm)\n",
    "        if call:\n",
    "            positionValue.append(s*np.exp((c-rf)*ttm)*norm.cdf(d1)-strike*np.exp(-rf*ttm)*norm.cdf(d2))\n",
    "        else:\n",
    "            positionValue.append(strike*np.exp(-rf*ttm)*norm.cdf(-d2)-s*np.exp((c-rf)*ttm)*norm.cdf(-d1))\n",
    "    # list can not times -1, convert to array\n",
    "    positionValue = np.array(positionValue)\n",
    "    return positionValue * data['Holding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy in strategies:\n",
    "    data=df.query(\"Portfolio == @strategy\")\n",
    "    positionValue=data.apply(gbsm_plus,prices=prices,axis=1)\n",
    "    payoff=data.apply(calPayoff,prices=prices,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Payoff Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calPayoff(data,prices):\n",
    "    '''\n",
    "    Calculate the payoff at expiration date according to different price\n",
    "    '''\n",
    "    if data['Type']!='Option':\n",
    "        return prices*data['Holding']\n",
    "    call=True if data['OptionType']=='Call' else False\n",
    "    strike=data['Strike']\n",
    "    payoff=[]\n",
    "    for s in prices:\n",
    "        if call:\n",
    "            payoff.append(max(s-strike,0))\n",
    "        else:\n",
    "            payoff.append(max(strike-s,0))\n",
    "    # list can not times -1, convert to array\n",
    "    payoff = np.array(payoff)\n",
    "    return payoff * data['Holding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy in strategies:\n",
    "    data=df.query(\"Portfolio == @strategy\")\n",
    "    positionValue=data.apply(gbsm_plus,prices=prices,axis=1)\n",
    "    payoff=data.apply(calPayoff,prices=prices,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the AR1 Model and simulate the return 10 days ahead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the AR1 Model and simulate the return 10 days ahead \n",
    "priceData=pd.read_csv('DailyPrices.csv',index_col='Date')\n",
    "logReturn=return_calculate(priceData.AAPL,option=\"CONTINUOUS\",rm_means=True)\n",
    "# fit AR1 model\n",
    "mod = sm.tsa.arima.ARIMA(logReturn.values, order=(1, 0, 0))\n",
    "start_params = [0.5,0.1]\n",
    "res = mod.fit_constrained({'const':0},start_params=start_params)\n",
    "# current Price\n",
    "currentPrice=151.03\n",
    "# simulate price ten days later\n",
    "simulatedPrice=[]\n",
    "N=100000 # number of simulations\n",
    "\n",
    "simulatedReturns=[]\n",
    "for _ in range(N):\n",
    "    simulatedReturn=res.simulate(nsimulations=10)\n",
    "    simulatedReturns.append(simulatedReturn)\n",
    "    tenDaysReturn=sum(simulatedReturn)\n",
    "    simulatedPrice.append(currentPrice*np.exp(tenDaysReturn))\n",
    "simulatedPrice=np.array(simulatedPrice)\n",
    "simulatedReturns=np.array(simulatedReturns).flatten()\n",
    "\n",
    "# apply those returns to the current AAPL price (above). Calculate Mean, VaR and ES\n",
    "strategies=df.Portfolio.unique()\n",
    "columns=['Mean(Portfolio Value)','Mean(Change)','VaR','ES']\n",
    "ans=pd.DataFrame(index=strategies,columns=columns)\n",
    "for idx,strategy in enumerate(strategies):\n",
    "    data=df.query(\"Portfolio == @strategy\")\n",
    "    # Simulated Position Value\n",
    "    positionValue=data.apply(gbsm_plus,prices=simulatedPrice,days=10,axis=1)\n",
    "    # Initial Position Value\n",
    "    iniValue=data.apply(gbsm_plus,prices=np.array([currentPrice]),axis=1)\n",
    "    # Simulated Portfolio Value\n",
    "    totalPortfolioValue=positionValue.sum()\n",
    "    # Initial Portfolio Value\n",
    "    iniValue=iniValue.sum()\n",
    "    # Changed Portfolio Value\n",
    "    valueChange=totalPortfolioValue-iniValue\n",
    "    valueChange = np.array(valueChange)\n",
    "    # calculate VaR and Expected Shortfall\n",
    "    VaR_p = RiskMetrics.VaR_historical(valueChange,alpha=0.05)\n",
    "    ES = RiskMetrics.ES_historical(valueChange,alpha=0.05)\n",
    "\n",
    "    # insert Mean, VaR, ES to dateframe\n",
    "    mean_var_es=[totalPortfolioValue.mean(),valueChange.mean(),VaR_p,ES]\n",
    "    ans.loc[strategy]=mean_var_es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greeks using closed formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greeks_closed_form(s,strike,ttm,vol,rf,c,call=True):\n",
    "    '''Closed from for greeks calculation from Generalize Black Scholes Merton\n",
    "        Generalize Black Scholes Merton:\n",
    "        rf = c       -- Black Scholes 1973\n",
    "        c = rf - q   -- Merton 1973 stock model where q is the continous dividend yield\n",
    "        c = 0        -- Black 1976 futures option model\n",
    "        c,r = 0      -- Asay 1982 margined futures option model\n",
    "        c = rf - rff -- Garman and Kohlhagen 1983 currency option model where rff is the risk free rate of the foreign currency\n",
    "\n",
    "        Option valuation via BSM closed formula\n",
    "        European Style.  Assumed LogNormal Prices\n",
    "        s - Underlying Price\n",
    "        strike - Strike Price\n",
    "        ttm - time to maturity\n",
    "        rf - Risk free rate\n",
    "        vol - Yearly Volatility\n",
    "        c - Cost of Carry\n",
    "        call - Call valuation if set True\n",
    "    '''\n",
    "    d1=(np.log(s/strike)+(c+vol**2/2)*ttm)/vol/np.sqrt(ttm)\n",
    "    d2=d1-vol*np.sqrt(ttm)\n",
    "    optionType=['Call'] if call else ['Put']\n",
    "    ans=pd.DataFrame(index=optionType,columns=['Detla','Gamma','Vega','Theta','Rho','Carry Rho'])\n",
    "    if call:\n",
    "        ans['Detla'] = np.exp((c-rf)*ttm)*norm.cdf(d1,loc=0,scale=1)\n",
    "        ans['Theta'] = -s*np.exp((c-rf)*ttm)*norm.pdf(d1,loc=0,scale=1)*vol/(2*np.sqrt(ttm))-(c-rf)*s*np.exp((c-rf)*ttm)*norm.cdf(d1,loc=0,scale=1)-rf*strike*np.exp(-rf*ttm)*norm.cdf(d2,loc=0,scale=1)\n",
    "        # ans['Rho'] = ttm*strike*np.exp(-rf*ttm)*norm.cdf(d2,loc=0,scale=1) - s*ttm*np.exp((c-rf)*ttm)*norm.cdf(d1,loc=0,scale=1)\n",
    "        ans['Rho'] = ttm*strike*np.exp(-rf*ttm)*norm.cdf(d2,loc=0,scale=1)\n",
    "\n",
    "        ans['Carry Rho'] = ttm*s*np.exp((c-rf)*ttm)*norm.cdf(d1,loc=0,scale=1)\n",
    "    else:\n",
    "        ans['Detla'] = np.exp((c-rf)*ttm)*(norm.cdf(d1,loc=0,scale=1)-1)\n",
    "        ans['Theta'] = -s*np.exp((c-rf)*ttm)*norm.pdf(d1,loc=0,scale=1)*vol/(2*np.sqrt(ttm))+(c-rf)*s*np.exp((c-rf)*ttm)*norm.cdf(-d1,loc=0,scale=1)+rf*strike*np.exp(-rf*ttm)*norm.cdf(-d2,loc=0,scale=1)\n",
    "        ans['Rho'] = -ttm*strike*np.exp(-rf*ttm)*norm.cdf(-d2,loc=0,scale=1)\n",
    "        # ans['Rho'] = -ttm*strike*np.exp(-rf*ttm)*norm.cdf(-d2,loc=0,scale=1)+ttm*s*norm.cdf(-d1,loc=0,scale=1)*exp((b-rf)*ttm)\n",
    "        ans['Carry Rho'] = -ttm*s*np.exp((c-rf)*ttm)*norm.cdf(-d1,loc=0,scale=1)\n",
    "    ans['Gamma'] = norm.pdf(d1,loc=0,scale=1)*np.exp((c-rf)*ttm)/(s*vol*np.sqrt(ttm))\n",
    "    ans['Vega'] = s*np.exp((c-rf)*ttm)*norm.pdf(d1,loc=0,scale=1)*np.sqrt(ttm)\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greeks for European option via Finite Difference (Generalize Black Scholes Merton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greeks_gbsm_finite_diff(s,strike,ttm,vol,rf,c,call=True):\n",
    "    '''Greeks for European option via Generalize Black Scholes Merton and finite difference'''\n",
    "    optionType=['Call'] if call else ['Put']\n",
    "    s_chg = 0.01 # amount of price change \n",
    "    vol_chg = 0.01 # amount of volatility change\n",
    "    t_chg = 0.01 # amount of ttm change\n",
    "    rf_chg = 0.01 # amount of rf change\n",
    "    c_chg = 0.01 # amount of carry of cost change\n",
    "\n",
    "    ans=pd.DataFrame(index=optionType,columns=['Detla','Gamma','Vega','Theta','Rho','Carry Rho'])\n",
    "\n",
    "    price_price_m_s_chg=gbsm(s-s_chg,strike,ttm,vol,rf,c,call) # price-s_chg\n",
    "    price_price_p_s_chg=gbsm(s+s_chg,strike,ttm,vol,rf,c,call) # price+s_chg\n",
    "    price=gbsm(s,strike,ttm,vol,rf,c,call)\n",
    "\n",
    "    ans['Detla']=(price_price_p_s_chg-price_price_m_s_chg)/(2*s_chg)\n",
    "    \n",
    "    ans['Gamma']=(price_price_p_s_chg+price_price_m_s_chg-2*price)/s_chg**2\n",
    "\n",
    "    ans['Vega']=(gbsm(s,strike,ttm,vol+vol_chg,rf,c,call)-gbsm(s,strike,ttm,vol-vol_chg,rf,c,call))/(2*vol_chg)\n",
    "\n",
    "    ans['Theta']=-(gbsm(s,strike,ttm+t_chg,vol,rf,c,call)-gbsm(s,strike,ttm-t_chg,vol,rf,c,call))/(2*t_chg)\n",
    "\n",
    "    ans['Rho']=(gbsm(s,strike,ttm,vol,rf+rf_chg,c+rf_chg,call)-gbsm(s,strike,ttm,vol,rf-rf_chg,c-rf_chg,call))/(2*rf_chg) # c should also be changed since c=rf-q \n",
    "\n",
    "    ans['Carry Rho']=(gbsm(s,strike,ttm,vol,rf,c+c_chg,call)-gbsm(s,strike,ttm,vol,rf,c-c_chg,call))/(2*c_chg)\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "CurrentDate=datetime.datetime.strptime('03/13/2022','%m/%d/%Y')\n",
    "ExpirationDate=datetime.datetime.strptime('04/15/2022','%m/%d/%Y')\n",
    "Dividend_date=datetime.datetime.strptime('04/11/2022','%m/%d/%Y')\n",
    "\n",
    "t=(ExpirationDate-CurrentDate).days\n",
    "tradingDayYear=365\n",
    "divTimes=(Dividend_date-CurrentDate).days\n",
    "\n",
    "s=165\n",
    "strike=165\n",
    "rf=0.0425\n",
    "q=0.0053\n",
    "c=rf-q\n",
    "ttm=t/tradingDayYear\n",
    "vol=0.2\n",
    "\n",
    "N=200\n",
    "divTimes=[divTimes/t*N]\n",
    "divAmts=[0.88]\n",
    "\n",
    "# European\n",
    "# Closed Form of Greeks \n",
    "call_greeks_closed_form=greeks_closed_form(s,strike,ttm,vol,rf,c,call=True)\n",
    "put_greeks_closed_form=greeks_closed_form(s,strike,ttm,vol,rf,c,call=False)\n",
    "# Finite Difference derivative calculation\n",
    "call_greeks_gbsm_finite_diff=greeks_gbsm_finite_diff(s,strike,ttm,vol,rf,c,call=True)\n",
    "put_greeks_gbsm_finite_diff=greeks_gbsm_finite_diff(s,strike,ttm,vol,rf,c,call=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## American Option Valuation via Binomial Tree (No dividends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial_tree_gbsm_american(s,strike,ttm,vol,rf,c,N=200,call=True):\n",
    "    ''' Generalize Black Scholes Merton\n",
    "        rf = c       -- Black Scholes 1973\n",
    "        c = rf - q   -- Merton 1973 stock model where q is the continous dividend yield\n",
    "        c = 0        -- Black 1976 futures option model\n",
    "        c,r = 0      -- Asay 1982 margined futures option model\n",
    "        c = rf - rff -- Garman and Kohlhagen 1983 currency option model where rff is the risk free rate of the foreign currency\n",
    "\n",
    "        Option valuation via Binomial Trees\n",
    "        European Style.  Assumed LogNormal Prices\n",
    "        s - Underlying Price\n",
    "        strike - Strike Price\n",
    "        t - days until maturity\n",
    "        rf - Risk free rate\n",
    "        vol - Yearly Volatility\n",
    "        c - Cost of Carry\n",
    "        tradingDayYear - trading days in a year\n",
    "        N - Steps of Binomial Tree\n",
    "        call - Call valuation if set True\n",
    "    '''\n",
    "    delta_t=ttm/N\n",
    "    # price multiplier in the (positive, negtive) case\n",
    "    u=np.exp(vol*np.sqrt(delta_t)) \n",
    "    d=np.exp(-vol*np.sqrt(delta_t))\n",
    "    # probability of up/down move in price\n",
    "    pu=(np.exp(c*delta_t)-d)/(u-d)\n",
    "    pd=1-pu\n",
    "    # discount factor\n",
    "    df=np.exp(-rf*delta_t)\n",
    "    # call or put\n",
    "    optionType=1 if call else -1\n",
    "\n",
    "    # American option\n",
    "    nNodeFunction = lambda x: (x+1)*(x+2)//2 # Calculate the number of all the nodes\n",
    "    # j: layer(step) of nodes (python start by 0)\n",
    "    #i: the i-th node in the j-th step (python start by 0)\n",
    "    idxFunction = lambda i,j: nNodeFunction(j-1)+i\n",
    "    nNodes=nNodeFunction(N) # Number of nodes\n",
    "    \n",
    "    optionValues = np.empty(nNodes,dtype=float) # An array of all the nodes\n",
    "\n",
    "    for j in range(N,-1,-1): \n",
    "        for i in range(j,-1,-1):\n",
    "            idx = idxFunction(i,j) \n",
    "            price = s*u**i*d**(j-i) # i represt how many times price go up\n",
    "            optionValues[idx]=max(0,optionType*(price-strike))\n",
    "            if j < N:\n",
    "                optionValues[idx]=max(optionValues[idx],df*(pu*optionValues[idxFunction(i+1,j+1)] + pd*optionValues[idxFunction(i,j+1)]))\n",
    "    return optionValues[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## American Option Valuation via Binomial Tree (with Dividends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf,c,N=200,call=True,divAmts=None,divTimes=None):\n",
    "    '''Generalize Black Scholes Merton\n",
    "        rf = c       -- Black Scholes 1973\n",
    "        c = rf - q   -- Merton 1973 stock model where q is the continous dividend yield\n",
    "        c = 0        -- Black 1976 futures option model\n",
    "        c,r = 0      -- Asay 1982 margined futures option model\n",
    "        c = rf - rff -- Garman and Kohlhagen 1983 currency option model where rff is the risk free rate of the foreign currency\n",
    "\n",
    "        Option valuation via Binomial Trees\n",
    "        European Style.  Assumed LogNormal Prices\n",
    "        s - Underlying Price\n",
    "        strike - Strike Price\n",
    "        t - days until maturity\n",
    "        rf - Risk free rate\n",
    "        vol - Yearly Volatility\n",
    "        c - Cost of Carry\n",
    "        tradingDayYear - trading days in a year\n",
    "        N - Steps of Binomial Tree\n",
    "        call - Call valuation if set True\n",
    "\n",
    "        divAmts - Array of dividend amounts\n",
    "        divTimes - Array of dividend times\n",
    "    '''\n",
    "    #if there are no dividends or the first dividend is outside out grid, return the standard bt_american value\n",
    "    divAmts=np.array(divAmts)\n",
    "    divTimes=np.array(divTimes).astype(np.int32)\n",
    "    if  divAmts.size == 0 or divTimes.size == 0 or divTimes[0]>N:\n",
    "        return binomial_tree_gbsm_american(s,strike,ttm,vol,rf,c,N,call)\n",
    "        \n",
    "    delta_t=ttm/N\n",
    "    # price multiplier in the (positive, negtive) case\n",
    "    u=np.exp(vol*np.sqrt(delta_t)) \n",
    "    d=np.exp(-vol*np.sqrt(delta_t))\n",
    "    # probability of up/down move in price\n",
    "    pu=(np.exp(c*delta_t)-d)/(u-d)\n",
    "    pd=1-pu\n",
    "    # discount factor\n",
    "    df=np.exp(-rf*delta_t)\n",
    "    # call or put\n",
    "    optionType=1 if call else -1\n",
    "\n",
    "    # American option\n",
    "    nNodeFunction = lambda x: (x+1)*(x+2)//2 # Calculate the number of all the nodes\n",
    "    # j: layer(step) of nodes (python start by 0)\n",
    "    #i: the i-th node in the j-th step (python start by 0)\n",
    "    idxFunction = lambda i,j: nNodeFunction(j-1)+i\n",
    "    nNodes=nNodeFunction(divTimes[0]) # Number of nodes\n",
    "\n",
    "    optionValues = np.empty(nNodes,dtype=float) # An array of all the nodes\n",
    "\n",
    "    for j in range(divTimes[0],-1,-1): \n",
    "        for i in range(j,-1,-1):\n",
    "            idx = idxFunction(i,j) \n",
    "            price = s*u**i*d**(j-i) # i represt how many times price go up\n",
    "            if j < divTimes[0]:\n",
    "                # times before the dividend working backward induction\n",
    "                optionValues[idx]=max(0,optionType*(price-strike))\n",
    "                optionValues[idx]=max(optionValues[idx],df*(pu*optionValues[idxFunction(i+1,j+1)] + pd*optionValues[idxFunction(i,j+1)]))\n",
    "            else:\n",
    "                # time of the dividend\n",
    "               valNoExercise = binomial_tree_gbsm_american_div(price-divAmts[0],strike,ttm-divTimes[0]*delta_t,vol,rf,c,N-divTimes[0],call,divAmts[1:],divTimes[1:]-divTimes[0])\n",
    "               valExercise =  max(0,optionType*(price-strike))\n",
    "               optionValues[idx] = max(valNoExercise,valExercise)\n",
    "    return optionValues[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valuation \n",
    "\n",
    "# Continuously Compounding Coupon\n",
    "# without dividend\n",
    "call_coupon_no_dividend = binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf,c,N,True,[],[])\n",
    "put_coupon_no_dividend = binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf,c,N,False,[],[])\n",
    "# With dividend\n",
    "call_coupon_dividend = binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf,c,N,True,divAmts,divTimes)\n",
    "put_coupon_dividend = binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf,c,N,False,divAmts,divTimes)\n",
    "\n",
    "# No Coupon\n",
    "# without dividend\n",
    "call_no_dividend = binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf,rf,N,True,[],[])\n",
    "put_no_dividend = binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf,rf,N,False,[],[])\n",
    "# With dividend\n",
    "call_dividend = binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf,rf,N,True,divAmts,divTimes)\n",
    "put_dividend = binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf,rf,N,False,divAmts,divTimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greeks for American/European Option via Binomial Tree and Finite Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greeks_binomial_tree_finite_diff(s,strike,ttm,vol,rf,c,N=200,call=True,divAmts=None,divTimes=None):\n",
    "    '''Greeks for American option via binomial tree and finite difference'''\n",
    "    optionType=['Call'] if call else ['Put']\n",
    "    s_chg = 0.2 # amount of price change \n",
    "    vol_chg = 0.01 # amount of volatility change\n",
    "    t_chg = 0.01 # amount of ttm change\n",
    "    rf_chg = 0.01 # amount of rf change\n",
    "    c_chg = 0.01 # amount of carry of cost change\n",
    "    divAmts_chg=0.01 # amount of dividend amount change\n",
    "\n",
    "    ans=pd.DataFrame(index=optionType,columns=['Detla','Gamma','Vega','Theta','Rho','Carry Rho','Sensitivity to Dividend Amount'])\n",
    "\n",
    "    price_price_m_s_chg=binomial_tree_gbsm_american_div(s-s_chg,strike,ttm,vol,rf,c,N,call,divAmts,divTimes) # price-s_chg\n",
    "    price_price_p_s_chg=binomial_tree_gbsm_american_div(s+s_chg,strike,ttm,vol,rf,c,N,call,divAmts,divTimes) # price+s_chg\n",
    "    price=binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf,c,N,call,divAmts,divTimes) \n",
    "\n",
    "    ans['Detla']=(price_price_p_s_chg-price_price_m_s_chg)/(2*s_chg)\n",
    "    \n",
    "    ans['Gamma']=(price_price_p_s_chg+price_price_m_s_chg-2*price)/s_chg**2\n",
    "\n",
    "    ans['Vega']=(binomial_tree_gbsm_american_div(s,strike,ttm,vol+vol_chg,rf,c,N,call,divAmts,divTimes)-binomial_tree_gbsm_american_div(s,strike,ttm,vol-vol_chg,rf,c,N,call,divAmts,divTimes))/(2*vol_chg)\n",
    "\n",
    "    ans['Theta']=-(binomial_tree_gbsm_american_div(s,strike,ttm+t_chg,vol,rf,c,N,call,divAmts,divTimes) - binomial_tree_gbsm_american_div(s,strike,ttm-t_chg,vol,rf,c,N,call,divAmts,divTimes) )/(2*t_chg)\n",
    "\n",
    "    ans['Rho']=(binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf+rf_chg,c+rf_chg,N,call,divAmts,divTimes) - binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf-rf_chg,c-rf_chg,N,call,divAmts,divTimes) )/(2*rf_chg) # c should also be changed since c=rf-q\n",
    "\n",
    "    ans['Carry Rho']=(binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf,c+c_chg,N,call,divAmts,divTimes)  - binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf,c-c_chg,N,call,divAmts,divTimes) )/(2*c_chg)\n",
    "\n",
    "    if divAmts and divTimes:\n",
    "        divAmts = np.array(divAmts)\n",
    "        divTimes = np.array(divTimes)\n",
    "        ans['Sensitivity to Dividend Amount'] = (binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf,c,N,call,divAmts+divAmts_chg,divTimes)  - binomial_tree_gbsm_american_div(s,strike,ttm,vol,rf,c,N,call,divAmts-divAmts_chg,divTimes) )/(2*divAmts_chg)\n",
    "\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greeks derived by binomial tree and finite difference\n",
    "\n",
    "# Continuously Compounding Coupon\n",
    "# without dividend (Continuously Compounding Coupon of 0.53%)\n",
    "call_greeks_coupon_no_dividend=greeks_binomial_tree_finite_diff(s,strike,ttm,vol,rf,c,N,True,[],[])\n",
    "put_greeks_coupon_no_dividend=greeks_binomial_tree_finite_diff(s,strike,ttm,vol,rf,c,N,False,[],[])\n",
    "# With dividend (Continuously Compounding Coupon of 0.53% + Discrete dividends)\n",
    "call_greeks_coupon_dividend=greeks_binomial_tree_finite_diff(s,strike,ttm,vol,rf,c,N,True,divAmts,divTimes)\n",
    "put_greeks_coupon_dividend=greeks_binomial_tree_finite_diff(s,strike,ttm,vol,rf,c,N,False,divAmts,divTimes)\n",
    "\n",
    "# No coupon\n",
    "# without dividend (No coupon)\n",
    "call_greeks_no_dividend=greeks_binomial_tree_finite_diff(s,strike,ttm,vol,rf,rf,N,True,[],[])\n",
    "put_greeks_no_dividend=greeks_binomial_tree_finite_diff(s,strike,ttm,vol,rf,rf,N,False,[],[])\n",
    "# With dividend (No coupon + Discrete dividends)\n",
    "call_greeks_dividend=greeks_binomial_tree_finite_diff(s,strike,ttm,vol,rf,rf,N,True,divAmts,divTimes)\n",
    "put_greeks_dividend=greeks_binomial_tree_finite_diff(s,strike,ttm,vol,rf,rf,N,False,divAmts,divTimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option Delta (could be applid to DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def option_delta(data):\n",
    "    '''Use binomial tree and finite difference to find the delta'''\n",
    "\n",
    "    if data['Type']!='Option': # if the security is not option return 1\n",
    "        return 1\n",
    "\n",
    "    s_chg=0.2\n",
    "\n",
    "    tradingDayYear=365\n",
    "\n",
    "    CurrentDate=datetime.datetime.strptime('03/03/2023','%m/%d/%Y')\n",
    "    ExpirationDate=data['ExpirationDate']\n",
    "    Dividend_date=datetime.datetime.strptime('03/15/2023','%m/%d/%Y')\n",
    "    ExpirationDate=datetime.datetime.strptime(ExpirationDate,'%m/%d/%Y')\n",
    "    t=(ExpirationDate-CurrentDate).days\n",
    "    divTimes=(Dividend_date-CurrentDate).days\n",
    "\n",
    "    s=151.03\n",
    "    strike=data['Strike']\n",
    "    call=True if data['OptionType']=='Call' else False\n",
    "    rf=0.0425\n",
    "    c=rf\n",
    "    ttm=t/tradingDayYear\n",
    "    P=data['CurrentPrice']\n",
    "    vol=data['Implied Volatility']\n",
    "\n",
    "    N=200\n",
    "    divTimes=[divTimes/t*N]\n",
    "    divAmts=[1]\n",
    "\n",
    "    price_price_m_s_chg=binomial_tree_gbsm_american_div(s-s_chg,strike,ttm,vol,rf,c,N,call,divAmts,divTimes) # price-s_chg\n",
    "    price_price_p_s_chg=binomial_tree_gbsm_american_div(s+s_chg,strike,ttm,vol,rf,c,N,call,divAmts,divTimes) # price+s_chg\n",
    "\n",
    "    ans=(price_price_p_s_chg-price_price_m_s_chg)/(2*s_chg)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Normal for Option Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('problem2.csv')\n",
    "# current stock price\n",
    "s=151.03\n",
    "# Calculate the implied volatility of each option\n",
    "iVol=df.apply(findImpliedVol_plus,axis=1)\n",
    "df['Implied Volatility']=iVol\n",
    "# Calculate the delta of each option\n",
    "deltas=df.apply(option_delta,axis=1)\n",
    "df['Delta']=deltas\n",
    "# Asset Value\n",
    "df['AssetValue']=df.apply(lambda x: abs(x['Holding'])*x['CurrentPrice'],axis=1)\n",
    "# Portfolio Value\n",
    "PortfolioValue=df.groupby('Portfolio').sum()['AssetValue']\n",
    "df['PortfolioValue']=df.apply(lambda x: PortfolioValue[x['Portfolio']],axis=1)\n",
    "# Weighted Delta\n",
    "df['WeightedDelta']=df.apply(lambda x: x['Holding']*s*x['Delta']/x['PortfolioValue'],axis=1)\n",
    "# df['WeightedDelta']=df.apply(lambda x: x['Holding']*s*x['Delta'],axis=1)\n",
    "\n",
    "# Calculate the return of AAPL\n",
    "priceData=pd.read_csv('DailyPrices.csv',index_col='Date')\n",
    "logReturn=return_calculate(priceData.AAPL,option=\"CONTINUOUS\",rm_means=True)\n",
    "# Calculate the Volatility of AAPL\n",
    "AAPLvol=logReturn.var()\n",
    "# Calculate the Volatility of Portfolio\n",
    "PortfolioStd=np.sqrt((df.groupby('Portfolio').sum()['WeightedDelta'])**2*AAPLvol*10)\n",
    "df['PortfolioStd']=df.apply(lambda x: PortfolioStd[x['Portfolio']],axis=1)\n",
    "\n",
    "\n",
    "ans=pd.DataFrame()\n",
    "alpha=0.05\n",
    "# Calculate the VaR of Portfolio (alpha=0.05)\n",
    "VaR=-norm.ppf(alpha,loc=0,scale=PortfolioStd)\n",
    "# Calculate the ES of Portfolio (alpha=0.05)\n",
    "ES=[]\n",
    "for std,var in zip(PortfolioStd,VaR):\n",
    "    ES.append(-norm(loc=0,scale=std).expect(lb=-np.inf,ub=var)/alpha)\n",
    "ES=np.array(ES)\n",
    "\n",
    "ans['VaR']=df.groupby('Portfolio')['PortfolioValue'].first()*VaR\n",
    "ans['ES']=df.groupby('Portfolio')['PortfolioValue'].first()*ES\n",
    "ans['Mean']=0\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super Efficient Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_efficient_portfolio(expected_rts,cov,rf=0.0425):\n",
    "    '''Given a target return, use assets to find the optimal portfolio with lowest risk'''\n",
    "    fun=lambda wts: -(wts@expected_rts-rf)/np.sqrt(wts@cov@wts)\n",
    "    x0 = np.full(expected_rts.shape[0],1/expected_rts.shape[0])\n",
    "    cons = [{'type':'ineq', 'fun':lambda x:x},\n",
    "        {'type':'eq', 'fun':lambda x:sum(x)-1}]\n",
    "    bounds = [(0, 1) for _ in range(expected_rts.shape[0])]\n",
    "    res = optimize.minimize(fun, x0, method='SLSQP',bounds=bounds,constraints=cons)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "# delta of stock is 1 * holdings\n",
    "delta_stock=1 * 1\n",
    "# delta of call option is delta(call) * holdings\n",
    "delta_call=call_greeks_closed_form['Detla'][0] * (-1)\n",
    "# Portfolio Delta \n",
    "delta_portfolio = delta_stock + delta_call\n",
    "\n",
    "# Delta Normal VaR is just the Portfolio Delta * quantile * current Underlying Price\n",
    "alpha=0.05\n",
    "VaR=abs(norm.ppf(alpha,loc=0,scale=vol))*delta_portfolio*s\n",
    "VaR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
